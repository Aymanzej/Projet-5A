{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b544bdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ df_model cr√©√© avec succ√®s.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Chargement du fichier CSV original\n",
    "# Remplacez par votre chemin : \"C:\\\\Users\\\\ZEJLI\\\\Projet 5A\\\\bdd_temperature\\\\df_clean.csv\"\n",
    "path_csv = \"df_clean.csv\" \n",
    "df_raw = pd.read_csv(path_csv, index_col=0)\n",
    "df_raw.index = pd.to_datetime(df_raw.index)\n",
    "\n",
    "# Cr√©ation de df_model avec feature engineering\n",
    "df_model = df_raw.copy()\n",
    "df_model = df_model.sort_values(['latitude', 'longitude', 'time'])\n",
    "\n",
    "# Features temporelles\n",
    "df_model['month'] = df_model.index.month\n",
    "df_model['month_sin'] = np.sin(2 * np.pi * df_model['month'] / 12)\n",
    "df_model['month_cos'] = np.cos(2 * np.pi * df_model['month'] / 12)\n",
    "\n",
    "# Index temporel pour le GP\n",
    "min_date = df_model.index.min()\n",
    "df_model['time_idx'] = (df_model.index.year - min_date.year) * 12 + (df_model.index.month - min_date.month)\n",
    "\n",
    "print(\"‚úÖ df_model cr√©√© avec succ√®s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3c11b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ D√©partements charg√©s : 63, 03, 15, 43\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import folium\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import branca.colormap as cm\n",
    "from cartiflette import carti_download\n",
    "\n",
    "# 1. Cr√©ation de l'arborescence de dossiers\n",
    "folders = [\"cartes_climat/historique\", \"cartes_climat/predictions_2008\"]\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "# 2. T√©l√©chargement et filtrage des 4 d√©partements (Auvergne)\n",
    "deps_codes = ['63', '03', '15', '43']\n",
    "departements_fr = carti_download(\n",
    "    values=[\"France\"],\n",
    "    crs=4326,\n",
    "    borders=\"DEPARTEMENT\",\n",
    "    vectorfile_format=\"geojson\",\n",
    "    simplification=50,\n",
    "    filter_by=\"FRANCE_ENTIERE\",\n",
    "    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n",
    "    year=2022\n",
    ")\n",
    "\n",
    "auvergne_deps = departements_fr[departements_fr['INSEE_DEP'].isin(deps_codes)].copy()\n",
    "print(f\"‚úÖ D√©partements charg√©s : {', '.join(deps_codes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50d9e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion du DataFrame global en GeoDataFrame\n",
    "gdf_total = gpd.GeoDataFrame(\n",
    "    df_model, # Utilise le dataframe de l'√©tape pr√©c√©dente\n",
    "    geometry=gpd.points_from_xy(df_model['longitude'], df_model['latitude']),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "# Jointure spatiale pour ne garder que les points des 4 d√©partements\n",
    "gdf_auvergne = gpd.sjoin(gdf_total, auvergne_deps[['INSEE_DEP', 'geometry']], predicate=\"within\")\n",
    "\n",
    "# S√©paration Historique / Pr√©diction\n",
    "df_hist = gdf_auvergne[gdf_auvergne.index.year <= 2007].copy()\n",
    "# df_2008 doit contenir vos colonnes 'skt' (v√©rit√©) et 'pred_gp' ou 'pred_lstm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b6f234b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ df_train et df_test_raw sont pr√™ts (28896 points d'entra√Ænement).\n"
     ]
    }
   ],
   "source": [
    "# 1. S√©paration temporelle (Train: 2000-2007, Test: 2008)\n",
    "# On utilise gdf_auvergne qui contient les 4 d√©partements\n",
    "df_train = gdf_auvergne[gdf_auvergne.index.year <= 2007].copy()\n",
    "df_test_raw = gdf_auvergne[gdf_auvergne.index.year == 2008].copy()\n",
    "\n",
    "# 2. Normalisation (Obligatoire pour le bon fonctionnement du GP)\n",
    "scaler_skt = StandardScaler()\n",
    "scaler_geo = StandardScaler()\n",
    "\n",
    "df_train['skt_norm'] = scaler_skt.fit_transform(df_train[['skt']])\n",
    "df_train[['lat_norm', 'lon_norm']] = scaler_geo.fit_transform(df_train[['latitude', 'longitude']])\n",
    "\n",
    "# On applique les r√©glages du train sur le test\n",
    "df_test_raw['skt_norm'] = scaler_skt.transform(df_test_raw[['skt']])\n",
    "df_test_raw[['lat_norm', 'lon_norm']] = scaler_geo.transform(df_test_raw[['latitude', 'longitude']])\n",
    "\n",
    "print(f\"‚úÖ df_train et df_test_raw sont pr√™ts ({len(df_train)} points d'entra√Ænement).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b395bc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Entra√Ænement du GP stabilis√©...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZEJLI\\anaconda3\\envs\\Transformers\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py:663: ConvergenceWarning: lbfgs failed to converge after 0 iteration(s) (status=2):\n",
      "ABNORMAL: \n",
      "\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  _check_optimize_result(\"lbfgs\", opt_res)\n",
      "c:\\Users\\ZEJLI\\anaconda3\\envs\\Transformers\\lib\\site-packages\\sklearn\\gaussian_process\\kernels.py:440: ConvergenceWarning: The optimal value found for dimension 0 of parameter k2__noise_level is close to the specified lower bound 1e-05. Decreasing the bound and calling fit again may find a better value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Mod√®le entra√Æn√© avec succ√®s !\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel, ExpSineSquared, ConstantKernel\n",
    "\n",
    "# 1. Pr√©paration des donn√©es et suppression des doublons exacts\n",
    "# Si deux points ont la m√™me position au m√™me moment, le GP plante.\n",
    "df_train_clean = df_train.drop_duplicates(subset=['time_idx', 'lat_norm', 'lon_norm'])\n",
    "\n",
    "# 2. √âchantillonnage\n",
    "df_sample = df_train_clean.sample(n=min(len(df_train_clean), 1500), random_state=42)\n",
    "\n",
    "# 3. Nouveau Kernel avec param√®tres de base plus larges\n",
    "kernel = (\n",
    "    ConstantKernel(1.0, (1e-3, 1e3)) * ExpSineSquared(periodicity=12.0, length_scale=1.0) + \n",
    "    Matern(length_scale=1.0, nu=1.5) + \n",
    "    WhiteKernel(noise_level=0.1)\n",
    ")\n",
    "\n",
    "# --- LA CORRECTION EST ICI : alpha=1e-2 ---\n",
    "# alpha ajoute une petite valeur sur la diagonale de la matrice pour la stabiliser\n",
    "gp = GaussianProcessRegressor(\n",
    "    kernel=kernel, \n",
    "    alpha=1e-2, \n",
    "    n_restarts_optimizer=3, \n",
    "    normalize_y=True\n",
    ")\n",
    "\n",
    "print(\"üöÄ Entra√Ænement du GP stabilis√©...\")\n",
    "try:\n",
    "    gp.fit(df_sample[['time_idx', 'lat_norm', 'lon_norm']].values, df_sample['skt_norm'].values)\n",
    "    print(\"‚úÖ Mod√®le entra√Æn√© avec succ√®s !\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Nouvelle erreur : {e}\")\n",
    "\n",
    "# 4. Pr√©diction pour 2008\n",
    "X_test = df_test_raw[['time_idx', 'lat_norm', 'lon_norm']].values\n",
    "y_pred_norm = gp.predict(X_test)\n",
    "\n",
    "df_2008 = df_test_raw.copy()\n",
    "df_2008['pred_gp'] = scaler_skt.inverse_transform(y_pred_norm.reshape(-1, 1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c43005ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Filtrage Z-score termin√©. Points conserv√©s : 3612\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# On filtre les valeurs aberrantes sur le r√©el et la pr√©diction\n",
    "# On garde les donn√©es √† l'int√©rieur de 3 √©cart-types\n",
    "df_2008 = df_2008.copy()\n",
    "df_2008['z_skt'] = np.abs(stats.zscore(df_2008['skt']))\n",
    "df_2008['z_pred'] = np.abs(stats.zscore(df_2008['pred_gp']))\n",
    "df_filtered_2008 = df_2008[(df_2008['z_skt'] < 3) & (df_2008['z_pred'] < 3)].copy()\n",
    "\n",
    "print(f\"‚úÖ Filtrage Z-score termin√©. Points conserv√©s : {len(df_filtered_2008)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cad0f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import branca.colormap as cm\n",
    "from scipy import stats\n",
    "\n",
    "def generate_pro_map(data, date_label, col_name, output_path, title_text):\n",
    "    # 1. Fond de carte BLANC (Positron)\n",
    "    m = folium.Map(location=[45.7, 3.2], zoom_start=8, tiles=\"CartoDB positron\")\n",
    "    \n",
    "    # 2. CONVERSION DES BORNES POUR L'√âCHELLE (Kelvin -> Celsius)\n",
    "    # On soustrait 273.15 pour que l'√©chelle affiche des valeurs r√©elles en ¬∞C\n",
    "    vmin_c = data[col_name].min() - 273.15\n",
    "    vmax_c = data[col_name].max() - 273.15\n",
    "    \n",
    "    # √âchelle Bleu -> Jaune -> Rouge (RdYlBu invers√©)\n",
    "    colors = cm.linear.RdYlBu_11.colors[::-1]\n",
    "    colormap = cm.LinearColormap(colors=colors, vmin=vmin_c, vmax=vmax_c)\n",
    "    colormap.caption = f\"Temp√©rature (¬∞C)\"\n",
    "    colormap.add_to(m)\n",
    "\n",
    "    # 3. √âchantillonnage 1/2 pour la clart√©\n",
    "    sampled_data = data.iloc[::2]\n",
    "\n",
    "    # 4. Contours des d√©partements (Auvergne)\n",
    "    folium.GeoJson(\n",
    "        auvergne_deps,\n",
    "        style_function=lambda x: {'fillColor': 'none', 'color': '#333', 'weight': 1.5, 'opacity': 0.4}\n",
    "    ).add_to(m)\n",
    "\n",
    "    # 5. Dessin des points INDIVIDUELS\n",
    "    for _, row in sampled_data.iterrows():\n",
    "        # --- CONVERSION DU POINT INDIVIDUEL ---\n",
    "        val_k = row[col_name]\n",
    "        val_c = val_k - 273.15  # Soustraction pour passer en Celsius\n",
    "        \n",
    "        # On utilise la valeur en Celsius pour obtenir la couleur correcte\n",
    "        color_point = colormap(val_c)\n",
    "        \n",
    "        popup_html = f\"\"\"\n",
    "        <div style=\"font-family: Arial; font-size: 12px; width: 160px;\">\n",
    "            <h4 style=\"margin:0; color:#333;\">{title_text}</h4>\n",
    "            <hr style=\"margin:5px 0;\">\n",
    "            <b>Mois :</b> {date_label}<br>\n",
    "            <b>Temp√©rature :</b> <b style=\"color:{color_point};\">{val_c:.2f} ¬∞C</b><br>\n",
    "            <b>Position :</b> {row['latitude']:.2f}, {row['longitude']:.2f}\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            location=[row['latitude'], row['longitude']],\n",
    "            radius=5,\n",
    "            color=color_point, # Contour du cercle\n",
    "            fill=True,\n",
    "            fill_color=color_point, # Remplissage du cercle\n",
    "            fill_opacity=0.8,\n",
    "            stroke=True,\n",
    "            weight=0.8,\n",
    "            popup=folium.Popup(popup_html, max_width=250)\n",
    "        ).add_to(m)\n",
    "\n",
    "    # 6. Titre de la carte\n",
    "    title_html = f'''\n",
    "             <div style=\"position: fixed; top: 10px; left: 50px; width: 300px; z-index:9999; \n",
    "                         background-color: white; border:2px solid black; padding: 10px; border-radius:5px;\">\n",
    "                 <b>{title_text}</b><br>P√©riode : {date_label}\n",
    "             </div>\n",
    "             '''\n",
    "    m.get_root().html.add_child(folium.Element(title_html))\n",
    "    m.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6903b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ G√©n√©ration Historique...\n",
      "‚è≥ G√©n√©ration Pr√©dictions 2008...\n",
      "‚ú® Termin√© ! Vos cartes sont pr√™tes et vari√©es en couleurs.\n"
     ]
    }
   ],
   "source": [
    "# --- √âTAPE 1 : Nettoyage Historique (2000-2007) ---\n",
    "df_hist['z_score'] = np.abs(stats.zscore(df_hist['skt']))\n",
    "df_hist_clean = df_hist[df_hist['z_score'] < 3].copy()\n",
    "df_hist_clean['year_month'] = df_hist_clean.index.to_period('M')\n",
    "\n",
    "print(\"‚è≥ G√©n√©ration Historique...\")\n",
    "for period, group in df_hist_clean.groupby('year_month'):\n",
    "    monthly = group.groupby(['latitude', 'longitude'])[['skt']].mean().reset_index()\n",
    "    generate_pro_map(monthly, str(period), 'skt', f\"cartes_climat/historique/hist_{period}.html\", \"Historique Auvergne\")\n",
    "\n",
    "# --- √âTAPE 2 : Nettoyage et Production Pr√©diction (2008) ---\n",
    "df_filtered_2008 = df_2008[(np.abs(stats.zscore(df_2008['skt'])) < 3)].copy()\n",
    "df_filtered_2008['period'] = df_filtered_2008.index.to_period('M')\n",
    "\n",
    "print(\"‚è≥ G√©n√©ration Pr√©dictions 2008...\")\n",
    "for p, group in df_filtered_2008.groupby('period'):\n",
    "    monthly = group.groupby(['latitude', 'longitude'])[['skt', 'pred_gp']].mean().reset_index()\n",
    "    \n",
    "    # Carte PR√âDITE\n",
    "    generate_pro_map(monthly, str(p), 'pred_gp', f\"cartes_climat/predictions_2008/MAP_PRED_{p}.html\", \"Pr√©diction IA\")\n",
    "    # Carte R√âELLE\n",
    "    generate_pro_map(monthly, str(p), 'skt', f\"cartes_climat/predictions_2008/MAP_REEL_{p}.html\", \"R√©alit√© Terrain\")\n",
    "\n",
    "print(\"‚ú® Termin√© ! Vos cartes sont pr√™tes et vari√©es en couleurs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3aab1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Nettoyage et g√©n√©ration des cartes historiques (2000-2007)...\n",
      "   -> Ann√©e 2000 en cours...\n",
      "   -> Ann√©e 2001 en cours...\n",
      "   -> Ann√©e 2002 en cours...\n",
      "   -> Ann√©e 2003 en cours...\n",
      "   -> Ann√©e 2004 en cours...\n",
      "   -> Ann√©e 2005 en cours...\n",
      "   -> Ann√©e 2006 en cours...\n",
      "   -> Ann√©e 2007 en cours...\n",
      "‚úÖ Cartes historiques termin√©es avec succ√®s dans /historique\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚è≥ Nettoyage et g√©n√©ration des cartes historiques (2000-2007)...\")\n",
    "\n",
    "# 1. Nettoyage Z-score sur l'historique\n",
    "# On enl√®ve les valeurs aberrantes pour que l'√©chelle de couleurs soit propre\n",
    "df_hist['z_score'] = np.abs(stats.zscore(df_hist['skt']))\n",
    "df_hist_clean = df_hist[df_hist['z_score'] < 3].copy()\n",
    "\n",
    "# 2. Cr√©ation de la colonne de p√©riode (Mois)\n",
    "df_hist_clean['year_month'] = df_hist_clean.index.to_period('M')\n",
    "\n",
    "# 3. Boucle de g√©n√©ration\n",
    "for period, group in df_hist_clean.groupby('year_month'):\n",
    "    # Calcul de la moyenne spatiale mensuelle\n",
    "    monthly_points = group.groupby(['latitude', 'longitude'])[['skt']].mean().reset_index()\n",
    "    \n",
    "    file_name = f\"cartes_climat/historique/hist_{period}.html\"\n",
    "    \n",
    "    # Appel de la nouvelle fonction \"Pro\"\n",
    "    generate_pro_map(\n",
    "        monthly_points, \n",
    "        str(period), \n",
    "        'skt', \n",
    "        file_name, \n",
    "        \"Historique Auvergne (ERA5)\"\n",
    "    )\n",
    "    \n",
    "    # Optionnel : petit print pour suivre l'avanc√©e car c'est long\n",
    "    if str(period).endswith(\"-01\"): \n",
    "        print(f\"   -> Ann√©e {str(period)[:4]} en cours...\")\n",
    "\n",
    "print(\"‚úÖ Cartes historiques termin√©es avec succ√®s dans /historique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ad1ab51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ G√©n√©ration des cartes en cours...\n",
      "‚úÖ Termin√© ! Les fichiers sont dans /predictions_2008\n"
     ]
    }
   ],
   "source": [
    "# Pr√©paration des p√©riodes\n",
    "df_filtered_2008['period'] = df_filtered_2008.index.to_period('M')\n",
    "\n",
    "print(\"‚è≥ G√©n√©ration des cartes en cours...\")\n",
    "\n",
    "for p, group in df_filtered_2008.groupby('period'):\n",
    "    # Moyenne par point pour le mois\n",
    "    monthly = group.groupby(['latitude', 'longitude'])[['skt', 'pred_gp']].mean().reset_index()\n",
    "    \n",
    "    # 1. Carte PR√âDITE\n",
    "    path_pred = f\"cartes_climat/predictions_2008/MAP_PRED_{p}.html\"\n",
    "    generate_pro_map(monthly, str(p), 'pred_gp', path_pred, \"PR√âDICTION IA\")\n",
    "    \n",
    "    # 2. Carte R√âELLE\n",
    "    path_reel = f\"cartes_climat/predictions_2008/MAP_REEL_{p}.html\"\n",
    "    generate_pro_map(monthly, str(p), 'skt', path_reel, \"R√âALIT√â TERRAIN\")\n",
    "\n",
    "print(f\"‚úÖ Termin√© ! Les fichiers sont dans /predictions_2008\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a84d31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
