<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr" xml:lang="fr">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    <title>Rapport Scientifique - Modélisation Forestière</title>
    <link rel="stylesheet" href="style.css">

    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            }
        };
    </script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>

<body>

    <header id="title-block-header">
        <p class="institution">Polytech Clermont-Ferrand — IMDS 5A</p>
        <h1 class="title">Modélisation de l’évolution temporelle des espèces forestières par apprentissage automatique
        </h1>
        <p class="author"><strong>Ayman ZEJLI</strong> &amp; <strong>Loïc MAGNAN</strong></p>
        <p class="author">Tuteur : <strong>Chafik SAMIR</strong></p>

        <div class="logo-wrapper">
            <img src="img/polytech_logo.png" alt="Logo Polytech Clermont" class="large-logo">
            <img src="img/imds_logo.png" alt="Logo IMDS" class="large-logo">
        </div>
    </header>

    <nav id="TOC">
        <h2 id="toc-title">Table des matières</h2>
        <ul>
            <li><a href="#introduction">1. Introduction</a></li>
            <li><a href="#fondamentaux">2. Approche de modélisation et fondamentaux</a>
                <ul>
                    <li><a href="#gp-presentation">2.1 Formalisme théorique des Processus Gaussiens</a></li>
                    <li><a href="#etude-experimentale">2.2 Étude expérimentale et comparaison de modèles</a>
                        <ul>
                            <li><a href="#fonctions-simple">2.2.1 Analyse sur fonction périodique simple : $y =
                                    \cos(x)$</a></li>
                            <li><a href="#fonctions-complexes">2.2.2 Analyse sur fonction complexe : $y = \cos(x) + x^2
                                    - 20\sin(5x)$</a></li>
                            <li><a href="#haute-complexite">2.2.3 Analyse sur fonction de haute complexité : $y = \ln(x)
                                    + 4x\cos(x^2)$</a></li>
                            <li><a href="#synthese-1d">2.2.4 Synthèse et bilan de l'étude 1D</a></li>
                        </ul>
                    </li>
                    <li><a href="#modelisation-2d">2.3 Modélisation spatiale et extension en dimension 2 (2D)</a>
                        <ul>
                            <li><a href="#architecture-cnn-2d">2.3.1 Structure et Approche du Réseau de Neurones (CNN
                                    2D)</a></li>
                            <li><a href="#2d-f1">2.3.2 Fonction simple : $f_1(x, y) = \cos(2\pi(x + y))$</a></li>
                            <li><a href="#2d-f2">2.3.3 Fonction intermédiaire : $f_2(x, y) = \sin(2\pi x) \cdot
                                    \cos(2\pi y)$</a></li>
                            <li><a href="#2d-f3">2.3.4 Fonction complexe : $f_3(x, y) = \sin(3\pi x)\cos(2\pi y) +
                                    e^{-5((x-0.5)^2 + (y-0.5)^2)}$</a></li>
                            <li><a href="#synthese-2d">2.3.5 Synthèse comparative des performances en 2D et
                                    transition</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li><a href="#application-forestiere">3. Application aux écosystèmes forestiers : Étude de cas réelle</a>
                <ul>
                    <li><a href="#acquisition-donnees">3.1 Acquisition et description du jeu de données ERA5-Land</a>
                    </li>
                    <li><a href="#ingenierie-donnees">3.2 Ingénierie des données et visualisations cartographiques</a>
                        <ul>
                            <li><a href="#bibliotheques-python">3.2.1 Écosystème logiciel pour l'analyse spatiale</a>
                            </li>
                            <li><a href="#heatmaps-thermales">3.2.2 Méthodologie des Heatmaps et délimitation
                                    spatiale</a></li>
                            <li><a href="#representation-features">3.2.3 Représentation des variables SKT et SKT_C
                                    (Année 2000)</a></li>
                            <li><a href="#moyenne-minmax">3.2.4 Analyse de la moyenne min-max par coordonnée</a></li>
                            <li><a href="#gradient-temporel">3.2.5 Gradient de température : Évolution 2000 vs 2007</a>
                            </li>
                        </ul>
                    </li>
                </ul>
            </li>
        </ul>
    </nav>

    <main>
        <h2 id="introduction">1. Introduction</h2>
        <p>L’analyse de la dynamique forestière face aux mutations climatiques contemporaines représente un défi majeur
            pour la gestion durable des ressources naturelles. Ce projet propose de modéliser l’évolution temporelle des
            espèces arborées en exploitant des données de terrain hétérogènes. En couplant les techniques
            d'apprentissage automatique aux Systèmes d’Information Géographique (SIG), nous analysons l'impact de
            facteurs critiques tels que la température, la sécheresse et l'âge des arbres pour anticiper les mutations
            des écosystèmes.</p>

        <h2 id="fondamentaux">2. Approche de modélisation et fondamentaux</h2>

        <h3 id="gp-presentation">2.1 Formalisme théorique des Processus Gaussiens</h3>
        <p>Le processus gaussien (GP) constitue une généralisation du concept de loi normale appliquée aux fonctions
            continues. Alors qu’une loi normale classique décrit la distribution d’un scalaire ou d’un vecteur aléatoire
            fini, le processus gaussien définit une distribution de probabilité sur un ensemble de fonctions. Cette
            approche bayésienne non paramétrique permet de modéliser la forme probable d’une fonction tout en
            quantifiant précisément l’incertitude associée à chaque prédiction.</p>

        <p>Un processus gaussien est intégralement caractérisé par deux fonctions fondamentales :</p>
        <ul>
            <li>La <strong>fonction moyenne</strong> $m(x)$, qui définit l'espérance de la fonction en tout point :
                $m(x) = \mathbb{E}[f(x)]$. Dans la pratique, on suppose souvent $m(x) = 0$ par défaut.</li>
            <li>La <strong>fonction de covariance</strong> $k(x, x')$, ou noyau (<em>Kernel</em>), qui modélise la
                dépendance statistique entre deux points : $k(x, x') = \text{Cov}(f(x), f(x'))$.</li>
        </ul>

        <p>La relation formelle s'écrit alors : $$f(x) \sim \mathcal{GP}(m(x), k(x, x'))$$</p>
        <p>Pour tout ensemble fini de points d’entrée $X = [x_1, \dots, x_n]$, le vecteur aléatoire correspondant $f =
            [f(x_1), \dots, f(x_n)]^T$ suit une <strong>loi normale multivariée</strong> : $$f \sim \mathcal{N}(m(X),
            K(X, X))$$ où $K$ est la matrice de Gram telle que $K_{ij} = k(x_i, x_j)$. Cette structure permet au modèle
            de calculer une distribution <em>a posteriori</em>, offrant une prédiction ponctuelle accompagnée d'une
            variance prédictive.</p>

        <h3 id="etude-experimentale">2.2 Étude expérimentale et comparaison de modèles en dimension 1</h3>

        <h4 id="fonctions-simple">2.2.1 Analyse sur fonction périodique simple : $y = \cos(x)$</h4>
        <p>Afin d'illustrer la capacité des processus gaussiens à reconstruire une fonction continue à partir d'un
            nombre limité d'observations, nous avons utilisé un modèle en une dimension pour approximer la fonction
            $f(x) = \cos(x)$. Pour cette approche, le modèle a été entraîné sur seulement 20 points choisis
            aléatoirement parmi 100 points générés uniformément dans l’intervalle $[0, 10]$. Les prédictions ont ensuite
            été effectuées sur l'ensemble des 100 points pour évaluer la qualité de l'interpolation par rapport au
            signal originel.</p>

        <p>Dans une seconde approche, un réseau de neurones simple en une dimension a été utilisé pour approximer la
            même fonction. Ce modèle, de type MLP (Multi-Layer Perceptron), est composé de deux couches cachées de 64
            neurones chacune. L'entraînement a été réalisé sur 100 points générés aléatoirement durant 300 époques avec
            un <em>batch size</em> de 8. Les prédictions obtenues sur un intervalle de test de 100 points permettent de
            visualiser la capacité du réseau à généraliser la fonction cosinus à partir des données disponibles, sans
            connaissance <em>a priori</em> de sa régularité.</p>

        <div class="side-by-side">
            <div class="fig-box">
                <img src="img/gp_1d_cos.png" alt="GP sur Cosinus" class="plot-img">
                <p class="caption"><strong>Figure 1 :</strong> Prédiction d’un processus gaussien 1D pour
                    l’approximation de la fonction cosinus sur l'intervalle [0, 10].</p>
            </div>
            <div class="fig-box">
                <img src="img/nn_1d_cos.png" alt="NN sur Cosinus" class="plot-img">
                <p class="caption"><strong>Figure 2 :</strong> Approximation de la fonction cosinus à l’aide d’un réseau
                    de neurones (300 époques, batch size 8).</p>
            </div>
        </div>

        <p>L'interprétation visuelle des résultats met en évidence une supériorité notable du modèle probabiliste dans
            ce scénario de données restreintes. Comme l'illustre la <strong>Figure 1</strong>, le processus gaussien
            parvient à reconstruire la fonction de manière quasi-parfaite malgré le faible nombre de points
            d'entraînement (20 points). L'aspect lisse de la courbe démontre que le noyau a correctement capturé la
            structure périodique du signal sur l'intervalle étudié.</p>

        <p>À l'inverse, la <strong>Figure 2</strong> révèle les limites de l'approche par réseau de neurones face à cet
            échantillonnage. Bien que la tendance globale soit respectée, la courbe présente des irrégularités locales.
            Contrairement au GP qui interpole par nature via sa structure de covariance, le réseau de neurones approxime
            la fonction par optimisation de poids. Sans une densité de points très élevée, il peine à égaler la fluidité
            du processus gaussien, confirmant la robustesse de ce dernier pour des signaux périodiques lisses avec peu
            de données.</p>

        <h4 id="fonctions-complexes">2.2.2 Analyse sur fonction complexe : $y = \cos(x) + x^2 - 20\sin(5x)$</h4>

        <p>Pour cette seconde phase expérimentale, nous testons la capacité des modèles à généraliser une structure
            complexe à partir d'un échantillonnage partiel. Le protocole consiste à sélectionner aléatoirement
            $n_{train}$ points parmi un ensemble de $N$ points générés, puis à utiliser ces modèles pour prédire la
            fonction sur l'intégralité du domaine. L'objectif est d'évaluer la précision de la reconstruction via
            l'Erreur Quadratique Moyenne (MSE).</p>

        <p>Dans le cadre des Processus Gaussiens, le choix du noyau (<em>kernel</em>) est déterminant car il définit les
            corrélations a priori entre les points. Nous avons testé quatre types de noyaux pour capturer les
            différentes composantes de notre fonction cible :</p>

        <ul>
            <li><strong>Radial Basis Function (RBF) :</strong> Le noyau par défaut, supposant une fluidité infinie.
                $$k(x, x') = \exp\left(-\frac{d(x, x')^2}{2l^2}\right)$$
            </li>
            <li><strong>Rational Quadratic :</strong> Équivalent à une somme de noyaux RBF avec différentes longueurs
                d'échelle, idéal pour des données variant à plusieurs échelles.
                $$k(x, x') = \left(1 + \frac{d(x, x')^2}{2\alpha l^2}\right)^{-\alpha}$$
            </li>
            <li><strong>Exp-Sine-Squared :</strong> Conçu pour capturer des périodicités strictes.
                $$k(x, x') = \exp\left(-\frac{2\sin^2(\pi d(x, x')/p)}{l^2}\right)$$
            </li>
            <li><strong>Matérn :</strong> Une généralisation du RBF permettant de modéliser des fonctions moins lisses,
                contrôlée par le paramètre $\nu$.
                $$k(x, x') = \frac{1}{\Gamma(\nu)2^{\nu-1}}\left(\frac{\sqrt{2\nu}}{l}d(x, x')\right)^\nu
                K_\nu\left(\frac{\sqrt{2\nu}}{l}d(x, x')\right)$$
            </li>
        </ul>

        <p>Les résultats de ces tests montrent que la flexibilité du noyau <strong>Rational Quadratic</strong> ou
            l'utilisation du <strong>Matérn</strong> permettent une meilleure adaptation aux oscillations rapides
            induites par le terme $-20\sin(5x)$ que le RBF classique. La <strong>Figure 3</strong> présente la
            prédiction optimale obtenue via le Processus Gaussien.</p>

        <div class="side-by-side" style="justify-content: center;">
            <div class="fig-box" style="max-width: 85%;">
                <img src="img/gp_1d_complex.png" alt="Prédiction GP avec différents kernels" class="plot-img">
                <p class="caption"><strong>Figure 3 :</strong> Comparaison des prédictions des kernels GP sur $y =
                    \cos(x) + x^2 - 20\sin(5x)$</p>
            </div>
        </div>

        <p>À l'inverse, l'approche par <strong>Réseau de Neurones</strong> (MLP) aborde ce problème sans hypothèse
            géométrique préalable. Le modèle doit apprendre la tendance quadratique et les oscillations simultanément à
            travers l'optimisation de ses poids. Pour cette fonction complexe, une étape de
            <strong>normalisation</strong> a été indispensable : les entrées $x$ ont été ramenées dans l'intervalle $[0,
            1]$ par un <em>Min-Max Scaling</em>, tandis que les sorties $y$ ont subi une standardisation ($Z$-score)
            pour stabiliser la descente de gradient de l'optimiseur <strong>Adam</strong>.
        </p>

        <p>L'architecture retenue est un réseau profond composé de trois couches denses de respectivement <strong>256
                neurones</strong>. L'utilisation de la fonction d'activation <strong>tanh</strong> entre chaque couche
            permet de capturer les non-linéarités complexes. Bien que le modèle soit capable de minimiser l'erreur
            quadratique moyenne (MSE) sur l'ensemble des $N$ points, il nécessite une phase d'entraînement intensive de
            <strong>300 époques</strong> avec un <strong>batch size de 128</strong>.
        </p>

        <div class="side-by-side" style="justify-content: center;">
            <div class="fig-box" style="max-width: 85%;">
                <img src="img/nn_1d_f_complex1.png" alt="Prédiction NN sur fonction complexe" class="plot-img">
                <p class="caption"><strong>Figure 4 :</strong> Prédictions d’un réseau de neurones sur $y =
                    \cos(x) + x^2 - 20\sin(5x)$</p>
            </div>
        </div>

        <p>L'interprétation de la <strong>Figure 4</strong> met en lumière une limite structurelle du réseau de neurones
            face à des signaux multi-échelles. On constate que la prédiction épouse parfaitement la tendance de fond
            parabolique. Cependant, elle lisse presque intégralement les oscillations à haute fréquence induites par le
            terme sinusoïdal. En agissant comme un <strong>filtre passe-bas</strong>, le réseau privilégie la
            minimisation de l'erreur globale sur la composante de forte amplitude, négligeant la structure locale
            pourtant présente dans les points d'entraînement (points rouges). À l'inverse, le <strong>Processus
                Gaussien</strong>, grâce à un noyau adapté, parvient à conserver cette fidélité aux variations rapides
            du signal.</p>
    </main>

    <h4 id="haute-complexite">2.2.3 Analyse sur fonction de haute complexité : $y = \ln(x) + 4x\cos(x^2)$</h4>
    <p>Pour pousser l'évaluation à un niveau critique, nous introduisons une fonction présentant une fréquence non
        constante et une croissance logarithmique : $y = \ln(x) + 4x\cos(x^2)$. Ce signal est particulièrement difficile
        à modéliser car la densité et l'amplitude des oscillations augmentent de manière quadratique avec $x$, rendant
        l'interpolation extrêmement sensible au choix du modèle.</p>

    <p><strong>Étude comparative des noyaux (GP) :</strong> La <strong>Figure 5</strong> illustre la sensibilité du
        Processus
        Gaussien face à ce signal non stationnaire. Le noyau <em>ExpSineSquared</em> échoue totalement à capturer la
        dynamique
        car il suppose une périodicité fixe, alors que la fréquence ici s'accélère. Le noyau <em>RBF</em>, trop rigide,
        lisse
        les oscillations dès que la fréquence augmente. À l'inverse, le noyau <strong>RationalQuadratic</strong> offre
        la
        meilleure reconstruction visuelle, s'adaptant aux différentes échelles de variation. Le noyau
        <strong>Matern</strong>
        ($\nu=1.5$) capture bien les pics mais montre des signes de saturation sur les amplitudes extrêmes en fin
        d'intervalle.
    </p>

    <div class="side-by-side" style="justify-content: center;">
        <div class="fig-box" style="max-width: 90%;">
            <img src="img/gp_1d_c2.png" alt="Comparaison des kernels GP" class="plot-img">
            <p class="caption"><strong>Figure 5 :</strong> Analyse de l'influence des kernels GP sur une fonction à
                fréquence
                variable. On observe que seul le noyau RationalQuadratic (en bas à gauche) parvient à suivre
                l'accélération
                du signal.</p>
        </div>
    </div>

    <p><strong>Évaluation du Réseau de Neurones (NN) :</strong> En parallèle, le modèle ReLU implémenté (64 neurones) a
        été
        testé sur ce même signal. Comme le montre la <strong>Figure 6</strong>, le réseau est capable de saisir la
        tendance
        logarithmique ascendante (la ligne moyenne du signal), mais il est totalement incapable de modéliser les
        oscillations.
        L'utilisation de l'activation <strong>ReLU</strong>, associée à une architecture légère, contraint le modèle à
        une
        approximation trop simpliste qui ignore les composantes haute fréquence.</p>

    <div class="side-by-side" style="justify-content: center;">
        <div class="fig-box" style="max-width: 85%;">
            <img src="img/nn_1d_c2.png" alt="NN sur fonction haute complexité" class="plot-img">
            <p class="caption"><strong>Figure 6 :</strong> Prédiction du réseau de neurones. La courbe orange montre un
                lissage extrême (effet filtre passe-bas), ne capturant que la tendance de fond logarithmique.</p>
        </div>
    </div>

    <p>La comparaison visuelle est sans appel : le GP avec un noyau <strong>RationalQuadratic</strong> (Figure 5)
        surpasse largement le réseau de neurones (Figure 6) en termes de fidélité au signal. Bien que le NN soit
        beaucoup
        plus rapide à s'exécuter sur de grands jeux de données, il agit ici comme un simple lisseur de tendance. Pour la
        modélisation forestière, où les micro-variations temporelles sont essentielles, le GP reste l'outil de
        référence,
        malgré un coût de calcul plus élevé lié à l'inversion de la matrice de covariance du kernel.
    </p>

    <p>En conclusion, sur des signaux à fréquence variable et amplitude croissante, la flexibilité du GP permet une
        reconstruction structurelle là où le NN ReLU standard échoue par manque de profondeur ou de points
        d'entraînement
        massifs.</p>
    </main>


    <main>
        <h4 id="synthese-1d">2.2.4 Synthèse et bilan de l'étude 1D</h4>

        <p>Après avoir analysé individuellement chaque fonction, nous proposons ici une vue d'ensemble des performances.
            Cette comparaison permet de quantifier l'efficacité des modèles selon la densité de l'échantillonnage.</p>

        <h5 class="scenario-title">Premier scénario : Échantillonnage faible (20 points)</h5>
        <p>Ce test simule des situations où les relevés de terrain sont rares. On observe une nette supériorité des
            Processus Gaussiens pour l'interpolation précise.</p>

        <p class="table-caption">Résultats sur $y = \cos(x)$ (20 pts)</p>
        <table class="small-table">
            <thead>
                <tr>
                    <th>Modèle</th>
                    <th>Temps (s)</th>
                    <th>MSE</th>
                    <th>Param.</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>GP (RatQuad)</td>
                    <td>0.0284</td>
                    <td>$7.3 \times 10^{-12}$</td>
                    <td>2</td>
                </tr>
                <tr>
                    <td>GP (ExpSine)</td>
                    <td>0.0173</td>
                    <td>$7.3 \times 10^{-12}$</td>
                    <td>2</td>
                </tr>
                <tr>
                    <td><strong>NN (MLP)</strong></td>
                    <td><strong>3.004</strong></td>
                    <td><strong>0.149</strong></td>
                    <td><strong>4353</strong></td>
                </tr>
            </tbody>
        </table>

        <p>Cependant, dès que la fonction gagne en complexité, l'écart de performance se creuse, notamment sur la
            gestion des fréquences élevées.</p>

        <p class="table-caption">Résultats sur $y = \cos(x) + x^2 - 20\sin(5x)$ (20 pts)</p>
        <table class="small-table">
            <thead>
                <tr>
                    <th>Modèle</th>
                    <th>Temps (s)</th>
                    <th>MSE</th>
                    <th>Param.</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>GP (RatQuad)</td>
                    <td>0.0058</td>
                    <td>146.9</td>
                    <td>2</td>
                </tr>
                <tr>
                    <td>GP (RBF)</td>
                    <td>0.0008</td>
                    <td>101.1</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td><strong>NN (MLP)</strong></td>
                    <td><strong>2.961</strong></td>
                    <td><strong>217.3</strong></td>
                    <td><strong>4353</strong></td>
                </tr>
            </tbody>
        </table>

        <h5 class="scenario-title">Second scénario : Échantillonnage dense (200 points)</h5>
        <p>En augmentant le nombre de points, le Réseau de Neurones améliore sa précision, mais reste largement distancé
            par la rapidité d'exécution des Processus Gaussiens.</p>

        <p class="table-caption">Résultats sur $y = \ln(x) + \cos(x^2) \times 4x$ (200 pts)</p>
        <table class="small-table">
            <thead>
                <tr>
                    <th>Modèle</th>
                    <th>Temps (s)</th>
                    <th>MSE</th>
                    <th>Param.</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>GP (RatQuad)</td>
                    <td>0.188</td>
                    <td>5.49</td>
                    <td>2</td>
                </tr>
                <tr>
                    <td>GP (Matern)</td>
                    <td>0.0054</td>
                    <td>245.8</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td><strong>NN (MLP)</strong></td>
                    <td><strong>3.383</strong></td>
                    <td><strong>295.3</strong></td>
                    <td><strong>4353</strong></td>
                </tr>
            </tbody>
        </table>

        <p>D’après ces observations, on peut conclure que, pour la prédiction de fonctions, le processus gaussien se
            révèle globalement plus performant qu’un réseau de neurones. En effet, que ce soit en termes de temps de
            calcul ou d’erreur quadratique moyenne (MSE), le processus gaussien obtient de meilleurs résultats. Il
            serait certes possible d’augmenter la complexité du réseau de neurones en ajoutant des couches ou davantage
            de neurones, mais cela entraînerait inévitablement une hausse du temps de calcul, probablement pour une MSE
            moins satisfaisante et un nombre de paramètres nettement plus élevé que celui d’un processus gaussien.</p>

        <div class="transition-box">
            <strong>Transition vers la 2D :</strong> Bien que les Processus Gaussiens dominent en 1D, l'analyse
            forestière nécessite de prendre en compte plusieurs variables simultanées (coordonnées spatiales,
            température). Dans la section suivante, nous allons évaluer si cette supériorité se maintient lors du
            passage à des dimensions supérieures.
        </div>

    </main>

    <h3 id="modelisation-2d">2.3 Modélisation spatiale et extension en dimension 2 </h3>
    <p>Le passage à la dimension 2 est crucial pour notre étude forestière, car il permet de simuler la répartition
        spatiale des espèces sur une parcelle définie par des coordonnées $(x, y)$. Dans cette section, nous évaluons la
        capacité des modèles à reconstruire des surfaces continues à partir d'un échantillonnage aléatoire réparti sur
        un domaine unitaire $[0, 1] \times [0, 1]$.</p>

    <h4 id="architecture-cnn-2d">2.3.1 Structure et Approche du Réseau de Neurones Convolutif (CNN 2D)</h4>

    <p>Pour l'approximation de fonctions continues en deux dimensions, nous avons implémenté un réseau de neurones
        convolutif (CNN 2D). Contrairement à un réseau dense classique, le CNN est capable de capturer des dépendances
        spatiales grâce à ses filtres. Le modèle reçoit en entrée une grille où chaque pixel contient ses propres
        coordonnées normalisées $[x, y]$.</p>

    <p>L'architecture repose sur une première couche de 128 filtres ($4 \times 4$) avec une activation <i>tanh</i> pour
        extraire les motifs locaux complexes, suivie d'une couche de 32 filtres pour modéliser les interactions entre
        les axes. Enfin, une couche de sortie ($1 \times 1$) produit la valeur prédite. L'apprentissage est assuré par
        l'optimiseur Adam en minimisant l'erreur quadratique moyenne (MSE). Bien que cette méthode soit puissante, elle
        reste dépendante de la structure de la grille et peut générer des approximations moins fluides que les méthodes
        stochastiques.</p>


    <h4 id="2d-f1">2.3.2 Fonction simple : $f_1(x, y) = \cos(2\pi(x + y))$</h4>

    <div class="side-by-side">
        <div class="fig-box">
            <img src="img/gp_2d_f1_p.png" alt="Prédiction GP f1" class="plot-img">
            <p class="caption"><strong>Figure 7 :</strong> Reconstruction par Processus Gaussien.</p>
        </div>
        <div class="fig-box">
            <img src="img/nn_2d_f1_p.png" alt="Prédiction CNN f1" class="plot-img">
            <p class="caption"><strong>Figure 8 :</strong> Approximation par CNN 2D.</p>
        </div>
    </div>

    <p>Sur cette fonction périodique simple, le <strong>Processus Gaussien</strong> (Figure 7) offre une surface d'une
        régularité mathématique parfaite, épousant les crêtes diagonales sans aucune distorsion. En revanche,
        l'observation de la <strong>Figure 8</strong> (CNN) révèle des faiblesses structurelles : on note un aspect
        "haché" sur les bords de la surface et une instabilité dans les zones de transition. Le réseau peine à maintenir
        une courbure constante, créant des ondulations parasites là où le GP reste parfaitement lisse.</p>

    <h4 id="2d-f2">2.3.3 Fonction intermédiaire : $f_2(x, y) = \sin(2\pi x) \cdot \cos(2\pi y)$</h4>

    <div class="side-by-side">
        <div class="fig-box">
            <img src="img/gp_2d_f2_p.png" alt="Prédiction GP f2" class="plot-img">
            <p class="caption"><strong>Figure 9 :</strong> Prédiction GP capturant les extrema.</p>
        </div>
        <div class="fig-box">
            <img src="img/nn_2d_f2_p.png" alt="Prédiction CNN f2" class="plot-img">
            <p class="caption"><strong>Figure 10 :</strong> Approximation par CNN 2D de la structure complexe.</p>
        </div>
    </div>

    <p>Pour cette structure en "boîte à œufs", l'écart visuel est encore plus marqué. Le GP (Figure 9) reconstitue
        fidèlement l'amplitude des pics et des vallées. La prédiction du CNN (Figure 10) montre un effondrement notable
        de la dynamique du signal : les sommets sont "écrasés" et la surface semble subitement s'aplatir vers les bords.
        On observe également des artefacts géométriques (lignes de cassure) qui témoignent de la difficulté du réseau à
        généraliser la fonction trigonométrique en dehors des points d'entraînement directs.</p>

    <h4 id="2d-f3">2.3.4 Fonction complexe : $f_3(x, y) = \sin(3\pi x)\cos(2\pi y) + e^{-5((x-0.5)^2 + (y-0.5)^2)}$</h4>

    <div class="side-by-side">
        <div class="fig-box">
            <img src="img/gp_2d_f3_p.png" alt="Prédiction GP f3" class="plot-img">
            <p class="caption"><strong>Figure 11 :</strong> Reconstruction GP isolant le pic central.</p>
        </div>
        <div class="fig-box">
            <img src="img/nn_2d_f3_p.png" alt="Prédiction CNN f3" class="plot-img">
            <p class="caption"><strong>Figure 12 :</strong> Approximation CNN 2D de la fonction hybride.</p>
        </div>
    </div>

    <p>Cette fonction mélangeant oscillations et singularité centrale met en évidence la supériorité du GP (Figure 11),
        qui parvient à isoler le pic exponentiel tout en stabilisant le relief périphérique. La <strong>Figure
            12</strong> montre que le CNN échoue presque totalement à stabiliser la surface : le pic central est déformé
        et le relief environnant est noyé dans un bruit visuel important. Le réseau semble "perdu" entre la gestion du
        pic de forte amplitude et les micro-oscillations, produisant un rendu chaotique et inutilisable pour une
        cartographie de précision.</p>

    <h4 id="synthese-2d">2.3.5 Synthèse comparative des performances en 2D</h4>

    <p>Le tableau ci-dessous regroupe les métriques de performance obtenues pour les deux modèles sur les trois surfaces
        de test. Cette comparaison quantitative permet de valider les observations visuelles faites précédemment sur la
        fidélité de la reconstruction.</p>

    <table class="small-table">
        <thead>
            <tr>
                <th>Fonction</th>
                <th>Modèle</th>
                <th>MSE (Erreur)</th>
                <th>Score $R^2$</th>
                <th>Incertitude moyenne (GP)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td rowspan="2">$f_1$ : cosinus simple</td>
                <td><strong>Processus Gaussien</strong></td>
                <td>0.0002</td>
                <td>0.9996</td>
                <td>0.0090</td>
            </tr>
            <tr>
                <td>Réseau CNN</td>
                <td>0.0507</td>
                <td>0.8986</td>
                <td>/</td>
            </tr>
            <tr>
                <td rowspan="2">$f_2$ : boîte à œufs</td>
                <td><strong>Processus Gaussien</strong></td>
                <td>0.0001</td>
                <td>0.9996</td>
                <td>0.0056</td>
            </tr>
            <tr>
                <td>Réseau CNN</td>
                <td>0.0941</td>
                <td>0.6233</td>
                <td>/</td>
            </tr>
            <tr>
                <td rowspan="2">$f_3$ : pic + oscillations</td>
                <td><strong>Processus Gaussien</strong></td>
                <td>0.0037</td>
                <td>0.9874</td>
                <td>0.0191</td>
            </tr>
            <tr>
                <td>Réseau CNN</td>
                <td>0.1462</td>
                <td>0.4978</td>
                <td>/</td>
            </tr>
        </tbody>
    </table>

    <p>L'analyse de ces résultats confirme que le <strong>Processus Gaussien (GP)</strong> reste le modèle le plus
        performant pour la modélisation en dimension 2. Sur les fonctions $f_1$ et $f_2$, le GP atteint une précision
        quasi parfaite avec des scores $R^2$ de 0,9996, tandis que le réseau CNN montre des signes de faiblesse,
        notamment sur la fonction $f_2$ où son score chute à 0,6233.</p>

    <p>Le cas de la fonction complexe $f_3$ est le plus révélateur : alors que le Processus Gaussien maintient une
        excellente capacité d'approximation ($R^2 = 0,9874$), les performances du réseau CNN s'effondrent avec un score
        de 0,4978 et une erreur (MSE) nettement plus élevée (0,1462 contre 0,0037 pour le GP). Cette différence
        s'explique par la capacité du GP à gérer nativement les corrélations spatiales et les changements d'échelle via
        son noyau, là où le CNN peine à généraliser à partir de coordonnées brutes sur des signaux complexes.</p>

    <p>En conclusion de cette étude bidimensionnelle, le Processus Gaussien s'impose à nouveau comme l'outil de
        référence. Sa capacité à fournir une estimation de l'incertitude (très faible ici, entre 0,005 et 0,019) offre
        une garantie supplémentaire sur la fiabilité de la cartographie forestière générée, ce que ne permet pas
        l'architecture CNN utilisée.</p>

    <div class="transition-box">
        <p><strong>Transition vers l'application forestière :</strong> Cette étude comparative sur fonctions simulées a
            démontré la robustesse des Processus Gaussiens (GP) pour modéliser des signaux complexes avec une grande
            fluidité et une gestion fiable de l'incertitude. Forts de ces résultats théoriques, nous allons désormais
            appliquer ces modèles à des données forestières réelles. L'enjeu sera d'évaluer si la supériorité du GP se
            confirme face au bruit et à la variance de séries temporelles pluriannuelles (température et humidité), afin
            de fournir des prédictions robustes pour la préservation de ces écosystèmes fragiles.</p>
    </div>


    <h2 id="application-forestiere">3. Application aux écosystèmes forestiers : Étude de cas réelle</h2>

    <p>Après avoir validé la supériorité des Processus Gaussiens sur des fonctions mathématiques simulées, nous
        orientons désormais nos travaux vers une application concrète en écologie forestière. L'objectif est de
        confronter nos modèles à la complexité de données réelles de terrain, caractérisées par un bruit de mesure et
        une variabilité environnementale élevée.</p>

    <h3 id="acquisition-donnees">3.1 Acquisition et description du jeu de données ERA5-Land</h3>

    <p>Pour cette étude, nous exploitons la base de données <strong>ERA5-Land monthly averaged data</strong> du service
        Copernicus Climate Change Service (C3S). Ce jeu de données fournit une reconstruction globale de l'évolution du
        climat terrestre sur plusieurs décennies avec une résolution spatiale fine. Pour notre analyse, nous avons ciblé
        spécifiquement le département du <strong>Puy-de-Dôme (63)</strong> sur la période allant de <strong>2000 à
            2007 (cf. figure 13)</strong>.</p>

    <div class="side-by-side" style="justify-content: center;">
        <div class="fig-box" style="max-width: 85%;">
            <img src="img/site_temp.png" alt="Figure 13" style="width: 100%;">
            <p class="caption"><strong>Figure 13 :</strong> Portail du Climate Data Store (CDS) permettant l'accès aux
                réanalyses climatiques ERA5-Land.</p>
        </div>
    </div>

    <p>Afin d'automatiser et de garantir la reproductibilité de l'extraction, nous utilisons l'API <strong>Copernicus
            Climate Data Store (CDS)</strong> via un script Python dédié. Le code configure une requête client (cf.
        figure 14) qui définit précisément les paramètres de téléchargement : le type de produit (moyennes mensuelles),
        le format de sortie (NetCDF compressé en ZIP) et les coordonnées géographiques limitant la zone d'étude aux
        frontières du Puy-de-Dôme, définies par une boîte englobante (Nord : 45.7, Est : 3.2, Sud : 45.6, Ouest : 3.1).
    </p>

    <div class="side-by-side" style="justify-content: center;">
        <div class="fig-box" style="max-width: 85%;">
            <img src="img/requete_site.png" alt="Figure 14" style="width: 100%;">
            <p class="caption"><strong>Figure 14 :</strong> Interface de configuration de la requête sur le Climate Data
                Store montrant la sélection des variables et de la période temporelle.</p>
        </div>
    </div>

    <p>La requête permet de récupérer un ensemble complet de variables thermiques essentielles pour comprendre les
        interactions entre l'atmosphère et le sol forestier. Nous collectons notamment la température du point de rosée
        à 2 mètres, la température de l'air à 2 mètres, ainsi que la <strong>température de surface (Skin temperature -
            SKT)</strong>. En complément, les températures du sol sont extraites sur quatre niveaux de profondeur
        distincts (Soil temperature level 1 à 4), permettant une analyse verticale des échanges thermiques. Pour amorcer
        notre modélisation spatio-temporelle, nous nous concentrerons prioritairement sur la variable
        <strong>SKT</strong> à l'échelle départementale.
    </p>

    <p>Une fois la requête API traitée et le fichier converti, nous obtenons une base de données structurée au format
        CSV. Ce fichier constitue la matière première de notre analyse. Chaque ligne représente une mesure
        spatio-temporelle précise sur le département du Puy-de-Dôme.</p>

    <div class="side-by-side" style="justify-content: center;">
        <div class="fig-box" style="max-width: 95%;">
            <img src="img/csv_clean.png" alt="Aperçu du fichier CSV ERA5-Land" class="plot-img">
            <p class="caption"><strong>Figure 15 :</strong> Aperçu de la structure tabulaire des données climatiques
                extraites (Fichier CSV).</p>
        </div>
    </div>

    <p>Le jeu de données se décompose selon les colonnes techniques suivantes :</p>
    <ul>
        <li><strong>time :</strong> L'horodatage de la mesure, correspondant à un échantillonnage mensuel sur la période
            2000-2007.</li>
        <li><strong>latitude / longitude :</strong> Les coordonnées géographiques précises permettant de situer chaque
            relevé sur la carte du département.</li>
        <li><strong>skt (Skin Temperature) :</strong> La température de surface brute exprimée en Kelvin.</li>
        <li><strong>sd (Snow Depth) :</strong> La profondeur de neige (équivalent eau), paramètre influençant l'albédo
            et l'isolation du sol.</li>
        <li><strong>swvl1 (Volumetric soil water layer 1) :</strong> Le volume d'eau contenu dans la première couche de
            sol (0-7 cm), indicateur de l'humidité de surface.</li>
        <li><strong>skt_C :</strong> La température de surface convertie par nos soins en <strong>degrés
                Celsius</strong> pour une interprétation physique plus intuitive.</li>
    </ul>

    <p>Cet ensemble de données constitue le socle nécessaire pour engager les travaux d'ingénierie statistique
        détaillés dans la section suivante. Cette étape cruciale permettra d'identifier les tendances saisonnières et
        les anomalies thermiques locales à travers la génération de <strong>cartes de chaleur (Heatmaps)</strong>. Ces
        visualisations seront essentielles pour appréhender la distribution thermique sur la topographie du Puy-de-Dôme
        et analyser précisément le <strong>gradient de température</strong> au fil des années.</p>

    <p>Une fois cette structure spatio-temporelle bien établie, nous passerons à la phase de
        <strong>prédiction</strong>. Nous confronterons alors la robustesse des <strong>Processus Gaussiens</strong> à
        des architectures de réseaux de neurones plus complexes, intégrant notamment des couches <strong>LSTM (Long
            Short-Term Memory)</strong>. Ce choix technologique nous permettra de capturer les dépendances temporelles à
        long terme inhérentes aux cycles climatiques forestiers, afin d'affiner la précision de nos modèles face aux
        variations réelles du terrain.
    </p>




    <h3 id="ingenierie-donnees">3.2 Ingénierie des données et visualisations cartographiques</h2>

        <p>Afin d'appréhender la structure spatio-temporelle de notre jeu de données ERA5-Land, une étape cruciale
            d'ingénierie des données est nécessaire. Avant de procéder à la modélisation prédictive, nous transformons
            les
            relevés tabulaires en représentations géographiques explicites pour identifier les gradients thermiques
            régionaux.</p>


        <h4 id="bibliotheques-python">3.2.1 Écosystème logiciel pour l'analyse spatiale
    </h3>

    <p>Le traitement des données géographiques et la génération de visuels dynamiques reposent sur trois bibliothèques
        Python piliers. Leur utilisation conjointe permet de transformer des coordonnées brutes en une structure
        cartographique exploitable.</p>

    <ul>
        <li>
            <strong>Cartiflette :</strong> Ce module est une solution de récupération automatisée des fonds de carte
            officiels (IGN, INSEE).
            <i>Utilité dans le code :</i> Elle est utilisée via la fonction <code>carti_download</code> pour requêter
            directement les vecteurs géographiques des départements français au format GeoJSON. Cela nous permet de
            délimiter précisément la zone d'étude auvergnate (codes 03, 15, 43, 63) sans manipulation manuelle de
            fichiers externes.
        </li>
        <li>
            <strong>GeoPandas :</strong> Véritable extension de Pandas dédiée à la donnée vectorielle, cette
            bibliothèque introduit le concept de <i>GeoDataFrame</i>.
            <i>Utilité dans le code :</i> Elle assure la conversion des coordonnées tabulaires (latitude/longitude) en
            objets géométriques projetés. Elle est le moteur de la jointure spatiale (<code>sjoin</code>), une opération
            critique qui filtre nos points de mesure ERA5-Land pour ne conserver que ceux situés physiquement à
            l'intérieur des frontières administratives de l'Auvergne.
        </li>
        <li>
            <strong>Folium :</strong> S'appuyant sur la puissance de la bibliothèque JavaScript <i>Leaflet</i>, Folium
            assure la couche de visualisation interactive.
            <i>Utilité dans le code :</i> Elle permet de générer la carte finale (<code>folium.Map</code>) et d'y
            superposer nos données thermiques sous forme de marqueurs circulaires colorés. Grâce à l'intégration de
            <i>Branca</i> pour la gestion des colormaps, elle transforme nos calculs de moyennes min-max en un gradient
            visuel intuitif où chaque point de la forêt peut être interrogé via des pop-ups informatifs.
        </li>
    </ul>

    <h4 id="heatmaps-thermales">3.2.2 Cartes de chaleur et gradients de température</h3>

        <p>La méthodologie adoptée pour la création des <strong>Heatmaps</strong> repose sur un processus de raffinement
            des
            données brutes ERA5-Land afin de rendre les tendances climatiques intelligibles à l'échelle régionale. Pour
            optimiser la lisibilité des cartes et éviter une surcharge visuelle, nous appliquons une réduction de la
            densité
            de la grille par un échantillonnage d'un point sur deux. </p>

        <p>Le traitement statistique consiste à agréger les relevés temporels pour chaque coordonnée géographique : nous
            calculons la moyenne arithmétique entre les valeurs minimales et maximales de la température de surface
            (SKT)
            sur l'ensemble de l'année sélectionnée. Cette métrique "min-max" permet de lisser les variations
            saisonnières
            extrêmes tout en conservant une image fidèle du gradient thermique annuel. </p>

        <p>Pour garantir la précision géographique de l'étude, nous utilisons les capacités de jointure spatiale de
            <i>GeoPandas</i> afin de circonscrire l'analyse aux limites administratives officielles. Comme l'illustre la
            <strong>Figure 16</strong>, notre périmètre d'étude se concentre sur les quatre départements constituant la
            région Auvergne, offrant ainsi un cadre topographique varié (plaines et massifs montagneux) idéal pour
            tester la
            robustesse de nos modèles prédictifs.
        </p>

        <div class="side-by-side" style="justify-content: center;">
            <div class="fig-box" style="max-width: 70%;">
                <img src="img/63_departements.png" alt="Carte des départements de l'Auvergne" class="plot-img">
                <p class="caption"><strong>Figure 16 :</strong> Délimitation géographique de la zone d'étude comprenant
                    les départements de l'Allier (03), du Cantal (15), de la Haute-Loire (43) et du Puy-de-Dôme (63).
                </p>
            </div>
        </div>

        <h4 id="representation-features">3.2.3 Représentation des variables SKT et SKT_C (Année 2000)</h4>

        <p>Pour amorcer l'analyse spatiale, nous avons procédé à une première représentation de la température de
            surface
            brute (SKT) exprimée en Kelvin. Cette étape est essentielle pour valider la cohérence physique des données
            extraites du service Copernicus avant toute manipulation complexe. La procédure automatisée commence par le
            chargement du jeu de données nettoyé et l'isolement de l'année cible, ici l'an 2000, tout en opérant une
            réduction de la grille par un échantillonnage d'un point sur deux afin d'épurer le rendu visuel.</p>

        <p>Le cœur du traitement repose sur l'intégration des contours géographiques via la bibliothèque
            <i>Cartiflette</i>,
            qui nous permet de projeter et de filtrer précisément les données sur les quatre départements de l'ancienne
            région Auvergne (Allier, Cantal, Haute-Loire et Puy-de-Dôme). Grâce à l'utilisation de <i>GeoPandas</i>,
            nous
            réalisons une jointure spatiale (<code>sjoin</code>) pour ne conserver que les relevés situés à l'intérieur
            de
            ces frontières administratives. Nous calculons ensuite la valeur moyenne de la température de surface pour
            chaque coordonnée géographique, agrégeant ainsi les variations temporelles mensuelles en une donnée annuelle
            représentative.
        </p>

        <p>La visualisation finale est générée à l'aide de <i>Folium</i>, où chaque point de mesure est représenté par
            un
            marqueur circulaire dont la couleur est indexée sur une échelle thermique inversée (du bleu pour le froid au
            rouge pour le chaud). Cette carte interactive permet d'explorer les zones de chaleur par simple survol,
            affichant des pop-ups informatifs avec les coordonnées exactes et la température moyenne (cf. figure 17).
            Nous avons
            appliqué
            rigoureusement la même méthodologie pour la variable <strong>SKT_C (Celsius)</strong>, confirmant ainsi la
            validité de notre conversion mathématique ($T_{°C} = T_{K} - 273.15$) et assurant une base sémantique plus
            intuitive pour l'interprétation climatique ultérieure (cf. figure 18).</p>

        <div class="side-by-side">
            <div class="fig-box">
                <img src="img/heatmap_skt.png" alt="Heatmap SKT Kelvin" class="plot-img">
                <p class="caption"><strong>Figure 17 :</strong> Représentation de la variable SKT (Kelvin) sur la région
                    Auvergne en 2000.</p>
            </div>
            <div class="fig-box">
                <img src="img/heatmap_sktC.png" alt="Heatmap SKT Celsius" class="plot-img">
                <p class="caption"><strong>Figure 18 :</strong> Représentation de la variable SKT_C (Celsius) sur la
                    région
                    Auvergne en 2000.</p>
            </div>
        </div>

        <h4 id="moyenne-minmax">3.2.4 Analyse de la moyenne min-max par coordonnée</h4>

        <p>Afin d'obtenir une vision synthétique du climat régional pour l'année 2000, nous avons mis en œuvre une
            méthodologie d'agrégation statistique simplifiée mais robuste : le calcul de la moyenne min-max. Cette
            approche consiste à isoler, pour chaque point géographique, les valeurs extrêmes enregistrées sur l'année
            afin d'en extraire la valeur médiane via la formule $\frac{min + max}{2}$. Ce choix méthodologique est
            particulièrement pertinent dans une étude préliminaire, car il permet de gommer le bruit des fluctuations
            mensuelles mineures tout en capturant l'amplitude thermique réelle subie par les écosystèmes forestiers.</p>

        <p>Le processus de visualisation repose sur un échantillonnage spatial stratégique. Pour assurer une clarté
            optimale sur la carte interactive sans saturer l'information, nous avons appliqué une réduction de la
            densité de la grille d'origine en ne conservant qu'un point sur deux. Cette sélection permet de maintenir
            une résolution suffisante pour identifier les micro-climats locaux tout en offrant une lecture fluide du
            gradient thermique à l'échelle des quatre départements de l'Auvergne.</p>

        <p>La cartographie finale, illustrée par la <strong>Figure 19</strong>, utilise une <i>colormap</i> divergente
            où les tons chauds mettent en exergue les zones de température de surface élevée. Le recours à une jointure
            spatiale rigoureuse garantit que seuls les relevés contenus dans les frontières administratives officielles
            sont pris en compte, éliminant ainsi les points marginaux. Chaque marqueur ainsi positionné devient un
            vecteur d'information complet, permettant par un simple survol d'accéder aux coordonnées précises et à la
            moyenne thermique calculée, facilitant l'identification visuelle des couloirs de chaleur régionaux.</p>

        <div class="side-by-side" style="justify-content: center;">
            <div class="fig-box" style="max-width: 80%;">
                <img src="img/heatmap_sktC_minmax.png" alt="Heatmap Min-Max 2000" class="plot-img">
                <p class="caption"><strong>Figure 19 :</strong> Carte de la moyenne annuelle min-max pour SKT_C en 2000
                    avec délimitation départementale.</p>
            </div>
        </div>

        <h4 id="gradient-temporel">3.2.5 Gradient de température : Évolution 2000 vs 2007</h4>

        <p>Pour parachever cette phase d'ingénierie des données, nous avons procédé à une analyse comparative visant à
            isoler l'évolution thermique spatiale sur l'ensemble de la période d'étude. La spécificité de cette approche
            réside dans le calcul d'un <strong>gradient thermique ($\Delta$)</strong> par point géographique, obtenu en
            soustrayant les moyennes annuelles de deux années charnières : 2000 et 2007. Cette méthode permet de mettre
            en lumière les zones géographiques ayant subi les variations de température de surface les plus
            significatives au cours de ces sept années.</p>

        <p>La méthodologie conserve la rigueur des étapes précédentes, notamment l'échantillonnage d'un point sur deux
            pour garantir la lisibilité et le calcul de la moyenne min-max pour stabiliser les valeurs annuelles.
            L'innovation ici réside dans la fusion (<code>merge</code>) des données de l'an 2000 et de l'an 2007 par
            coordonnées, permettant de quantifier précisément l'écart thermique local. Pour la visualisation, nous avons
            opté pour une palette de couleurs hautement contrastée (du bleu sombre au rouge vif), facilitant
            l'identification immédiate du réchauffement ou du refroidissement relatif des parcelles forestières
            auvergnates.</p>

        <p>Comme l'illustre la <strong>Figure 20</strong>, le calcul du delta $2007 - 2000$ révèle la progression
            thermique globale, où les tons chauds marquent une augmentation de la température de surface sur la période.
            À l'inverse, la <strong>Figure 21</strong> présente la soustraction inverse ($2000 - 2007$), offrant une
            perspective complémentaire sur les zones de stabilité ou de retrait thermique.
        </p>

        <div class="side-by-side">
            <div class="fig-box">
                <img src="img/heatmap_sktC_2007_2000.png" alt="Gradient thermique 2007-2000" class="plot-img">
                <p class="caption"><strong>Figure 20 :</strong> Gradient thermique SKT_C ($\Delta$ 2007 - 2000) en
                    Auvergne avec échelle contrastée.</p>
            </div>
            <div class="fig-box">
                <img src="img/heatmap_sktC_2000_2007.png" alt="Différence thermique 2000-2007" class="plot-img">
                <p class="caption"><strong>Figure 21 :</strong> Différence thermique relative ($\Delta$ 2000 - 2007)
                    mettant en évidence les inversions de gradient.</p>
            </div>
        </div>

        <p><strong>Synthèse et perspectives de modélisation :</strong> L'analyse de ces cartographies révèle une
            hétérogénéité spatiale marquée du réchauffement de surface à travers l'Auvergne, identifiant ainsi des
            "couloirs de chaleur" où les zones forestières sont particulièrement exposées au stress thermique. En
            quantifiant précisément ces gradients entre 2000 et 2007, nous avons constitué une base de données
            temporelle riche, indispensable pour caractériser les tendances climatiques locales.</p>


        <div class="transition-box">
            <p><strong>Transition vers la modélisation :</strong> Ces indicateurs visuels marquent la fin de la phase
                descriptive et
                ouvrent la voie à la
                <strong>prédiction</strong>. L'enjeu sera désormais de confronter la robustesse des <strong>Processus
                    Gaussiens</strong>, dont nous avons prouvé l'efficacité pour l'interpolation spatiale en 2D, à des
                architectures de réseaux de neurones complexes. Nous intégrerons notamment des couches <strong>LSTM
                    (Long Short-Term Memory)</strong>, spécifiquement conçues pour capturer les dépendances
                séquentielles à long terme, afin de déterminer quel modèle parvient le mieux à anticiper les dynamiques
                thermiques futures de ces écosystèmes fragiles.
            </p>
        </div>

        <h3 id="modelisation-forestiere">3.3 Modélisation prédictive : Confrontation des Processus Gaussiens et des
            architectures LSTM</h3>

        <p>
            La phase précédente d'ingénierie des données a permis de transformer des mesures tabulaires brutes en une
            vision géographique cohérente, mettant en lumière des gradients thermiques et des hétérogénéités spatiales
            critiques sur le territoire auvergnat. Ce constat visuel du "stress environnemental" constitue le point de
            départ de notre démarche prédictive : il ne s'agit plus seulement de décrire le passé, mais d'anticiper les
            dynamiques futures de la température de surface (SKT).
        </p>

        <p>
            Pour relever ce défi, nous introduisons une confrontation entre deux paradigmes technologiques
            complémentaires :
        </p>

        <ul>
            <li>
                <strong>Les Processus Gaussiens (GP) :</strong> Forts de leur succès lors de nos tests théoriques en 2D,
                ils sont ici mobilisés pour leur capacité exceptionnelle à l'interpolation spatiale. Leur force réside
                dans la gestion rigoureuse de l'incertitude, permettant de "combler les vides" entre les stations de
                mesure avec une précision mathématique inégalée.
            </li>
            <li>
                <strong>Les Réseaux de Neurones LSTM (Long Short-Term Memory) :</strong> Contrairement aux modèles
                classiques, les couches LSTM sont spécifiquement conçues pour traiter des séquences temporelles. Nous
                les utilisons ici pour leur "mémoire" capable de capturer les cycles saisonniers et les dépendances à
                long terme au sein de nos séries chronologiques (2000-2007), là où des modèles statiques échoueraient à
                percevoir la continuité du changement climatique.
            </li>
        </ul>

        <p>
            Cette double approche vise à déterminer si la précision géométrique des GP surpasse la capacité
            d'apprentissage séquentiel des LSTM, ou si une combinaison de ces forces est nécessaire pour modéliser avec
            fiabilité l'évolution thermique de nos écosystèmes forestiers.
        </p>


        <footer>
            <div class="footer-content">
                <p>Polytech Clermont-Ferrand — Département Ingénierie Mathématique et Data Science (IMDS)</p>
                <p>Projet 5A — Année Universitaire 2025 / 2026</p>
            </div>
        </footer>

</body>

</html>