<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="fr" xml:lang="fr">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    <title>Rapport Scientifique - Modélisation Forestière</title>
    <link rel="stylesheet" href="style.css">

    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            }
        };
    </script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>

<body>

    <header id="title-block-header">
        <p class="institution">Polytech Clermont-Ferrand — IMDS 5A</p>
        <h1 class="title">Modélisation de l’évolution temporelle des espèces forestières par apprentissage automatique
        </h1>
        <p class="author"><strong>Ayman ZEJLI</strong> &amp; <strong>Loïc MAGNAN</strong></p>
        <p class="author">Tuteur : <strong>Chafik SAMIR</strong></p>

        <div class="logo-wrapper">
            <img src="img/polytech_logo.png" alt="Logo Polytech Clermont" class="large-logo">
            <img src="img/imds_logo.png" alt="Logo IMDS" class="large-logo">
        </div>
    </header>

    <nav id="TOC">
        <h2 id="toc-title">Table des matières</h2>
        <ul>
            <li><a href="#introduction">1. Introduction</a></li>
            <li><a href="#fondamentaux">2. Approche de modélisation et fondamentaux</a>
                <ul>
                    <li><a href="#gp-presentation">2.1 Formalisme théorique des Processus Gaussiens</a></li>
                    <li><a href="#etude-experimentale">2.2 Étude expérimentale et comparaison de modèles</a>
                        <ul>
                            <li><a href="#fonctions-simple">2.2.1 Analyse sur fonction périodique simple : $y =
                                    \cos(x)$</a></li>
                            <li><a href="#fonctions-complexes">2.2.2 Analyse sur fonction complexe : $y = \cos(x) + x^2
                                    - 20\sin(5x)$</a></li>
                            <li><a href="#haute-complexite">2.2.3 Analyse sur fonction de haute complexité : $y = \ln(x)
                                    + 4x\cos(x^2)$</a></li>
                            <li><a href="#synthese-1d">2.2.4 Synthèse et bilan de l'étude 1D</a></li>
                        </ul>
                    </li>
                    <li><a href="#modelisation-2d">2.3 Modélisation spatiale et extension en dimension 2 (2D)</a>
                        <ul>
                            <li><a href="#architecture-cnn-2d">2.3.1 Structure et Approche du Réseau de Neurones (CNN
                                    2D)</a></li>
                            <li><a href="#2d-f1">2.3.2 Fonction simple : $f_1(x, y) = \cos(2\pi(x + y))$</a></li>
                            <li><a href="#2d-f2">2.3.3 Fonction intermédiaire : $f_2(x, y) = \sin(2\pi x) \cdot
                                    \cos(2\pi y)$</a></li>
                            <li><a href="#2d-f3">2.3.4 Fonction complexe : $f_3(x, y) = \sin(3\pi x)\cos(2\pi y) +
                                    e^{-5((x-0.5)^2 + (y-0.5)^2)}$</a></li>
                            <li><a href="#synthese-2d">2.3.5 Synthèse comparative des performances en 2D et
                                    transition</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
        </ul>
    </nav>

    <main>
        <h2 id="introduction">1. Introduction</h2>
        <p>L’analyse de la dynamique forestière face aux mutations climatiques contemporaines représente un défi majeur
            pour la gestion durable des ressources naturelles. Ce projet propose de modéliser l’évolution temporelle des
            espèces arborées en exploitant des données de terrain hétérogènes. En couplant les techniques
            d'apprentissage automatique aux Systèmes d’Information Géographique (SIG), nous analysons l'impact de
            facteurs critiques tels que la température, la sécheresse et l'âge des arbres pour anticiper les mutations
            des écosystèmes.</p>

        <h2 id="fondamentaux">2. Approche de modélisation et fondamentaux</h2>

        <h3 id="gp-presentation">2.1 Formalisme théorique des Processus Gaussiens</h3>
        <p>Le processus gaussien (GP) constitue une généralisation du concept de loi normale appliquée aux fonctions
            continues. Alors qu’une loi normale classique décrit la distribution d’un scalaire ou d’un vecteur aléatoire
            fini, le processus gaussien définit une distribution de probabilité sur un ensemble de fonctions. Cette
            approche bayésienne non paramétrique permet de modéliser la forme probable d’une fonction tout en
            quantifiant précisément l’incertitude associée à chaque prédiction.</p>

        <p>Un processus gaussien est intégralement caractérisé par deux fonctions fondamentales :</p>
        <ul>
            <li>La <strong>fonction moyenne</strong> $m(x)$, qui définit l'espérance de la fonction en tout point :
                $m(x) = \mathbb{E}[f(x)]$. Dans la pratique, on suppose souvent $m(x) = 0$ par défaut.</li>
            <li>La <strong>fonction de covariance</strong> $k(x, x')$, ou noyau (<em>Kernel</em>), qui modélise la
                dépendance statistique entre deux points : $k(x, x') = \text{Cov}(f(x), f(x'))$.</li>
        </ul>

        <p>La relation formelle s'écrit alors : $$f(x) \sim \mathcal{GP}(m(x), k(x, x'))$$</p>
        <p>Pour tout ensemble fini de points d’entrée $X = [x_1, \dots, x_n]$, le vecteur aléatoire correspondant $f =
            [f(x_1), \dots, f(x_n)]^T$ suit une <strong>loi normale multivariée</strong> : $$f \sim \mathcal{N}(m(X),
            K(X, X))$$ où $K$ est la matrice de Gram telle que $K_{ij} = k(x_i, x_j)$. Cette structure permet au modèle
            de calculer une distribution <em>a posteriori</em>, offrant une prédiction ponctuelle accompagnée d'une
            variance prédictive.</p>

        <h3 id="etude-experimentale">2.2 Étude expérimentale et comparaison de modèles en dimension 1</h3>

        <h4 id="fonctions-simple">2.2.1 Analyse sur fonction périodique simple : $y = \cos(x)$</h4>
        <p>Afin d'illustrer la capacité des processus gaussiens à reconstruire une fonction continue à partir d'un
            nombre limité d'observations, nous avons utilisé un modèle en une dimension pour approximer la fonction
            $f(x) = \cos(x)$. Pour cette approche, le modèle a été entraîné sur seulement 20 points choisis
            aléatoirement parmi 100 points générés uniformément dans l’intervalle $[0, 10]$. Les prédictions ont ensuite
            été effectuées sur l'ensemble des 100 points pour évaluer la qualité de l'interpolation par rapport au
            signal originel.</p>

        <p>Dans une seconde approche, un réseau de neurones simple en une dimension a été utilisé pour approximer la
            même fonction. Ce modèle, de type MLP (Multi-Layer Perceptron), est composé de deux couches cachées de 64
            neurones chacune. L'entraînement a été réalisé sur 100 points générés aléatoirement durant 300 époques avec
            un <em>batch size</em> de 8. Les prédictions obtenues sur un intervalle de test de 100 points permettent de
            visualiser la capacité du réseau à généraliser la fonction cosinus à partir des données disponibles, sans
            connaissance <em>a priori</em> de sa régularité.</p>

        <div class="side-by-side">
            <div class="fig-box">
                <img src="img/gp_1d_cos.png" alt="GP sur Cosinus" class="plot-img">
                <p class="caption"><strong>Figure 1 :</strong> Prédiction d’un processus gaussien 1D pour
                    l’approximation de la fonction cosinus sur l'intervalle [0, 10].</p>
            </div>
            <div class="fig-box">
                <img src="img/nn_1d_cos.png" alt="NN sur Cosinus" class="plot-img">
                <p class="caption"><strong>Figure 2 :</strong> Approximation de la fonction cosinus à l’aide d’un réseau
                    de neurones (300 époques, batch size 8).</p>
            </div>
        </div>

        <p>L'interprétation visuelle des résultats met en évidence une supériorité notable du modèle probabiliste dans
            ce scénario de données restreintes. Comme l'illustre la <strong>Figure 1</strong>, le processus gaussien
            parvient à reconstruire la fonction de manière quasi-parfaite malgré le faible nombre de points
            d'entraînement (20 points). L'aspect lisse de la courbe démontre que le noyau a correctement capturé la
            structure périodique du signal sur l'intervalle étudié.</p>

        <p>À l'inverse, la <strong>Figure 2</strong> révèle les limites de l'approche par réseau de neurones face à cet
            échantillonnage. Bien que la tendance globale soit respectée, la courbe présente des irrégularités locales.
            Contrairement au GP qui interpole par nature via sa structure de covariance, le réseau de neurones approxime
            la fonction par optimisation de poids. Sans une densité de points très élevée, il peine à égaler la fluidité
            du processus gaussien, confirmant la robustesse de ce dernier pour des signaux périodiques lisses avec peu
            de données.</p>

        <h4 id="fonctions-complexes">2.2.2 Analyse sur fonction complexe : $y = \cos(x) + x^2 - 20\sin(5x)$</h4>

        <p>Pour cette seconde phase expérimentale, nous testons la capacité des modèles à généraliser une structure
            complexe à partir d'un échantillonnage partiel. Le protocole consiste à sélectionner aléatoirement
            $n_{train}$ points parmi un ensemble de $N$ points générés, puis à utiliser ces modèles pour prédire la
            fonction sur l'intégralité du domaine. L'objectif est d'évaluer la précision de la reconstruction via
            l'Erreur Quadratique Moyenne (MSE).</p>

        <p>Dans le cadre des Processus Gaussiens, le choix du noyau (<em>kernel</em>) est déterminant car il définit les
            corrélations a priori entre les points. Nous avons testé quatre types de noyaux pour capturer les
            différentes composantes de notre fonction cible :</p>

        <ul>
            <li><strong>Radial Basis Function (RBF) :</strong> Le noyau par défaut, supposant une fluidité infinie.
                $$k(x, x') = \exp\left(-\frac{d(x, x')^2}{2l^2}\right)$$
            </li>
            <li><strong>Rational Quadratic :</strong> Équivalent à une somme de noyaux RBF avec différentes longueurs
                d'échelle, idéal pour des données variant à plusieurs échelles.
                $$k(x, x') = \left(1 + \frac{d(x, x')^2}{2\alpha l^2}\right)^{-\alpha}$$
            </li>
            <li><strong>Exp-Sine-Squared :</strong> Conçu pour capturer des périodicités strictes.
                $$k(x, x') = \exp\left(-\frac{2\sin^2(\pi d(x, x')/p)}{l^2}\right)$$
            </li>
            <li><strong>Matérn :</strong> Une généralisation du RBF permettant de modéliser des fonctions moins lisses,
                contrôlée par le paramètre $\nu$.
                $$k(x, x') = \frac{1}{\Gamma(\nu)2^{\nu-1}}\left(\frac{\sqrt{2\nu}}{l}d(x, x')\right)^\nu
                K_\nu\left(\frac{\sqrt{2\nu}}{l}d(x, x')\right)$$
            </li>
        </ul>

        <p>Les résultats de ces tests montrent que la flexibilité du noyau <strong>Rational Quadratic</strong> ou
            l'utilisation du <strong>Matérn</strong> permettent une meilleure adaptation aux oscillations rapides
            induites par le terme $-20\sin(5x)$ que le RBF classique. La <strong>Figure 3</strong> présente la
            prédiction optimale obtenue via le Processus Gaussien.</p>

        <div class="side-by-side" style="justify-content: center;">
            <div class="fig-box" style="max-width: 85%;">
                <img src="img/gp_1d_complex.png" alt="Prédiction GP avec différents kernels" class="plot-img">
                <p class="caption"><strong>Figure 3 :</strong> Comparaison des prédictions des kernels GP sur $y =
                    \cos(x) + x^2 - 20\sin(5x)$</p>
            </div>
        </div>

        <p>À l'inverse, l'approche par <strong>Réseau de Neurones</strong> (MLP) aborde ce problème sans hypothèse
            géométrique préalable. Le modèle doit apprendre la tendance quadratique et les oscillations simultanément à
            travers l'optimisation de ses poids. Pour cette fonction complexe, une étape de
            <strong>normalisation</strong> a été indispensable : les entrées $x$ ont été ramenées dans l'intervalle $[0,
            1]$ par un <em>Min-Max Scaling</em>, tandis que les sorties $y$ ont subi une standardisation ($Z$-score)
            pour stabiliser la descente de gradient de l'optimiseur <strong>Adam</strong>.
        </p>

        <p>L'architecture retenue est un réseau profond composé de trois couches denses de respectivement <strong>256
                neurones</strong>. L'utilisation de la fonction d'activation <strong>tanh</strong> entre chaque couche
            permet de capturer les non-linéarités complexes. Bien que le modèle soit capable de minimiser l'erreur
            quadratique moyenne (MSE) sur l'ensemble des $N$ points, il nécessite une phase d'entraînement intensive de
            <strong>300 époques</strong> avec un <strong>batch size de 128</strong>.
        </p>

        <div class="side-by-side" style="justify-content: center;">
            <div class="fig-box" style="max-width: 85%;">
                <img src="img/nn_1d_f_complex1.png" alt="Prédiction NN sur fonction complexe" class="plot-img">
                <p class="caption"><strong>Figure 4 :</strong> Prédictions d’un réseau de neurones sur $y =
                    \cos(x) + x^2 - 20\sin(5x)$</p>
            </div>
        </div>

        <p>L'interprétation de la <strong>Figure 4</strong> met en lumière une limite structurelle du réseau de neurones
            face à des signaux multi-échelles. On constate que la prédiction épouse parfaitement la tendance de fond
            parabolique. Cependant, elle lisse presque intégralement les oscillations à haute fréquence induites par le
            terme sinusoïdal. En agissant comme un <strong>filtre passe-bas</strong>, le réseau privilégie la
            minimisation de l'erreur globale sur la composante de forte amplitude, négligeant la structure locale
            pourtant présente dans les points d'entraînement (points rouges). À l'inverse, le <strong>Processus
                Gaussien</strong>, grâce à un noyau adapté, parvient à conserver cette fidélité aux variations rapides
            du signal.</p>
    </main>

    <h4 id="haute-complexite">2.2.3 Analyse sur fonction de haute complexité : $y = \ln(x) + 4x\cos(x^2)$</h4>
    <p>Pour pousser l'évaluation à un niveau critique, nous introduisons une fonction présentant une fréquence non
        constante et une croissance logarithmique : $y = \ln(x) + 4x\cos(x^2)$. Ce signal est particulièrement difficile
        à modéliser car la densité et l'amplitude des oscillations augmentent de manière quadratique avec $x$, rendant
        l'interpolation extrêmement sensible au choix du modèle.</p>

    <p><strong>Étude comparative des noyaux (GP) :</strong> La <strong>Figure 5</strong> illustre la sensibilité du
        Processus
        Gaussien face à ce signal non stationnaire. Le noyau <em>ExpSineSquared</em> échoue totalement à capturer la
        dynamique
        car il suppose une périodicité fixe, alors que la fréquence ici s'accélère. Le noyau <em>RBF</em>, trop rigide,
        lisse
        les oscillations dès que la fréquence augmente. À l'inverse, le noyau <strong>RationalQuadratic</strong> offre
        la
        meilleure reconstruction visuelle, s'adaptant aux différentes échelles de variation. Le noyau
        <strong>Matern</strong>
        ($\nu=1.5$) capture bien les pics mais montre des signes de saturation sur les amplitudes extrêmes en fin
        d'intervalle.
    </p>

    <div class="side-by-side" style="justify-content: center;">
        <div class="fig-box" style="max-width: 90%;">
            <img src="img/gp_1d_c2.png" alt="Comparaison des kernels GP" class="plot-img">
            <p class="caption"><strong>Figure 5 :</strong> Analyse de l'influence des kernels GP sur une fonction à
                fréquence
                variable. On observe que seul le noyau RationalQuadratic (en bas à gauche) parvient à suivre
                l'accélération
                du signal.</p>
        </div>
    </div>

    <p><strong>Évaluation du Réseau de Neurones (NN) :</strong> En parallèle, le modèle ReLU implémenté (64 neurones) a
        été
        testé sur ce même signal. Comme le montre la <strong>Figure 6</strong>, le réseau est capable de saisir la
        tendance
        logarithmique ascendante (la ligne moyenne du signal), mais il est totalement incapable de modéliser les
        oscillations.
        L'utilisation de l'activation <strong>ReLU</strong>, associée à une architecture légère, contraint le modèle à
        une
        approximation trop simpliste qui ignore les composantes haute fréquence.</p>

    <div class="side-by-side" style="justify-content: center;">
        <div class="fig-box" style="max-width: 85%;">
            <img src="img/nn_1d_c2.png" alt="NN sur fonction haute complexité" class="plot-img">
            <p class="caption"><strong>Figure 6 :</strong> Prédiction du réseau de neurones. La courbe orange montre un
                lissage extrême (effet filtre passe-bas), ne capturant que la tendance de fond logarithmique.</p>
        </div>
    </div>

    <p>La comparaison visuelle est sans appel : le GP avec un noyau <strong>RationalQuadratic</strong> (Figure 5)
        surpasse largement le réseau de neurones (Figure 6) en termes de fidélité au signal. Bien que le NN soit
        beaucoup
        plus rapide à s'exécuter sur de grands jeux de données, il agit ici comme un simple lisseur de tendance. Pour la
        modélisation forestière, où les micro-variations temporelles sont essentielles, le GP reste l'outil de
        référence,
        malgré un coût de calcul plus élevé lié à l'inversion de la matrice de covariance du kernel.
    </p>

    <p>En conclusion, sur des signaux à fréquence variable et amplitude croissante, la flexibilité du GP permet une
        reconstruction structurelle là où le NN ReLU standard échoue par manque de profondeur ou de points
        d'entraînement
        massifs.</p>
    </main>


    <main>
        <h4 id="synthese-1d">2.2.4 Synthèse et bilan de l'étude 1D</h4>

        <p>Après avoir analysé individuellement chaque fonction, nous proposons ici une vue d'ensemble des performances.
            Cette comparaison permet de quantifier l'efficacité des modèles selon la densité de l'échantillonnage.</p>

        <h5 class="scenario-title">Premier scénario : Échantillonnage faible (20 points)</h5>
        <p>Ce test simule des situations où les relevés de terrain sont rares. On observe une nette supériorité des
            Processus Gaussiens pour l'interpolation précise.</p>

        <p class="table-caption">Résultats sur $y = \cos(x)$ (20 pts)</p>
        <table class="small-table">
            <thead>
                <tr>
                    <th>Modèle</th>
                    <th>Temps (s)</th>
                    <th>MSE</th>
                    <th>Param.</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>GP (RatQuad)</td>
                    <td>0.0284</td>
                    <td>$7.3 \times 10^{-12}$</td>
                    <td>2</td>
                </tr>
                <tr>
                    <td>GP (ExpSine)</td>
                    <td>0.0173</td>
                    <td>$7.3 \times 10^{-12}$</td>
                    <td>2</td>
                </tr>
                <tr>
                    <td><strong>NN (MLP)</strong></td>
                    <td><strong>3.004</strong></td>
                    <td><strong>0.149</strong></td>
                    <td><strong>4353</strong></td>
                </tr>
            </tbody>
        </table>

        <p>Cependant, dès que la fonction gagne en complexité, l'écart de performance se creuse, notamment sur la
            gestion des fréquences élevées.</p>

        <p class="table-caption">Résultats sur $y = \cos(x) + x^2 - 20\sin(5x)$ (20 pts)</p>
        <table class="small-table">
            <thead>
                <tr>
                    <th>Modèle</th>
                    <th>Temps (s)</th>
                    <th>MSE</th>
                    <th>Param.</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>GP (RatQuad)</td>
                    <td>0.0058</td>
                    <td>146.9</td>
                    <td>2</td>
                </tr>
                <tr>
                    <td>GP (RBF)</td>
                    <td>0.0008</td>
                    <td>101.1</td>
                    <td>0</td>
                </tr>
                <tr>
                    <td><strong>NN (MLP)</strong></td>
                    <td><strong>2.961</strong></td>
                    <td><strong>217.3</strong></td>
                    <td><strong>4353</strong></td>
                </tr>
            </tbody>
        </table>

        <h5 class="scenario-title">Second scénario : Échantillonnage dense (200 points)</h5>
        <p>En augmentant le nombre de points, le Réseau de Neurones améliore sa précision, mais reste largement distancé
            par la rapidité d'exécution des Processus Gaussiens.</p>

        <p class="table-caption">Résultats sur $y = \ln(x) + \cos(x^2) \times 4x$ (200 pts)</p>
        <table class="small-table">
            <thead>
                <tr>
                    <th>Modèle</th>
                    <th>Temps (s)</th>
                    <th>MSE</th>
                    <th>Param.</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>GP (RatQuad)</td>
                    <td>0.188</td>
                    <td>5.49</td>
                    <td>2</td>
                </tr>
                <tr>
                    <td>GP (Matern)</td>
                    <td>0.0054</td>
                    <td>245.8</td>
                    <td>1</td>
                </tr>
                <tr>
                    <td><strong>NN (MLP)</strong></td>
                    <td><strong>3.383</strong></td>
                    <td><strong>295.3</strong></td>
                    <td><strong>4353</strong></td>
                </tr>
            </tbody>
        </table>

        <p>D’après ces observations, on peut conclure que, pour la prédiction de fonctions, le processus gaussien se
            révèle globalement plus performant qu’un réseau de neurones. En effet, que ce soit en termes de temps de
            calcul ou d’erreur quadratique moyenne (MSE), le processus gaussien obtient de meilleurs résultats. Il
            serait certes possible d’augmenter la complexité du réseau de neurones en ajoutant des couches ou davantage
            de neurones, mais cela entraînerait inévitablement une hausse du temps de calcul, probablement pour une MSE
            moins satisfaisante et un nombre de paramètres nettement plus élevé que celui d’un processus gaussien.</p>

        <div class="transition-box">
            <strong>Transition vers la 2D :</strong> Bien que les Processus Gaussiens dominent en 1D, l'analyse
            forestière nécessite de prendre en compte plusieurs variables simultanées (coordonnées spatiales,
            température). Dans la section suivante, nous allons évaluer si cette supériorité se maintient lors du
            passage à des dimensions supérieures.
        </div>

    </main>

    <h3 id="modelisation-2d">2.3 Modélisation spatiale et extension en dimension 2 </h3>
    <p>Le passage à la dimension 2 est crucial pour notre étude forestière, car il permet de simuler la répartition
        spatiale des espèces sur une parcelle définie par des coordonnées $(x, y)$. Dans cette section, nous évaluons la
        capacité des modèles à reconstruire des surfaces continues à partir d'un échantillonnage aléatoire réparti sur
        un domaine unitaire $[0, 1] \times [0, 1]$.</p>

    <h4 id="architecture-cnn-2d">2.3.1 Structure et Approche du Réseau de Neurones Convolutif (CNN 2D)</h4>

    <p>Pour l'approximation de fonctions continues en deux dimensions, nous avons implémenté un réseau de neurones
        convolutif (CNN 2D). Contrairement à un réseau dense classique, le CNN est capable de capturer des dépendances
        spatiales grâce à ses filtres. Le modèle reçoit en entrée une grille où chaque pixel contient ses propres
        coordonnées normalisées $[x, y]$.</p>

    <p>L'architecture repose sur une première couche de 128 filtres ($4 \times 4$) avec une activation <i>tanh</i> pour
        extraire les motifs locaux complexes, suivie d'une couche de 32 filtres pour modéliser les interactions entre
        les axes. Enfin, une couche de sortie ($1 \times 1$) produit la valeur prédite. L'apprentissage est assuré par
        l'optimiseur Adam en minimisant l'erreur quadratique moyenne (MSE). Bien que cette méthode soit puissante, elle
        reste dépendante de la structure de la grille et peut générer des approximations moins fluides que les méthodes
        stochastiques.</p>


    <h4 id="2d-f1">2.3.2 Fonction simple : $f_1(x, y) = \cos(2\pi(x + y))$</h4>

    <div class="side-by-side">
        <div class="fig-box">
            <img src="img/gp_2d_f1_p.png" alt="Prédiction GP f1" class="plot-img">
            <p class="caption"><strong>Figure 7 :</strong> Reconstruction par Processus Gaussien.</p>
        </div>
        <div class="fig-box">
            <img src="img/nn_2d_f1_p.png" alt="Prédiction CNN f1" class="plot-img">
            <p class="caption"><strong>Figure 8 :</strong> Approximation par CNN 2D.</p>
        </div>
    </div>

    <p>Sur cette fonction périodique simple, le <strong>Processus Gaussien</strong> (Figure 7) offre une surface d'une
        régularité mathématique parfaite, épousant les crêtes diagonales sans aucune distorsion. En revanche,
        l'observation de la <strong>Figure 8</strong> (CNN) révèle des faiblesses structurelles : on note un aspect
        "haché" sur les bords de la surface et une instabilité dans les zones de transition. Le réseau peine à maintenir
        une courbure constante, créant des ondulations parasites là où le GP reste parfaitement lisse.</p>

    <h4 id="2d-f2">2.3.3 Fonction intermédiaire : $f_2(x, y) = \sin(2\pi x) \cdot \cos(2\pi y)$</h4>

    <div class="side-by-side">
        <div class="fig-box">
            <img src="img/gp_2d_f2_p.png" alt="Prédiction GP f2" class="plot-img">
            <p class="caption"><strong>Figure 9 :</strong> Prédiction GP capturant les extrema.</p>
        </div>
        <div class="fig-box">
            <img src="img/nn_2d_f2_p.png" alt="Prédiction CNN f2" class="plot-img">
            <p class="caption"><strong>Figure 10 :</strong> Approximation par CNN 2D de la structure complexe.</p>
        </div>
    </div>

    <p>Pour cette structure en "boîte à œufs", l'écart visuel est encore plus marqué. Le GP (Figure 9) reconstitue
        fidèlement l'amplitude des pics et des vallées. La prédiction du CNN (Figure 10) montre un effondrement notable
        de la dynamique du signal : les sommets sont "écrasés" et la surface semble subitement s'aplatir vers les bords.
        On observe également des artefacts géométriques (lignes de cassure) qui témoignent de la difficulté du réseau à
        généraliser la fonction trigonométrique en dehors des points d'entraînement directs.</p>

    <h4 id="2d-f3">2.3.4 Fonction complexe : $f_3(x, y) = \sin(3\pi x)\cos(2\pi y) + e^{-5((x-0.5)^2 + (y-0.5)^2)}$</h4>

    <div class="side-by-side">
        <div class="fig-box">
            <img src="img/gp_2d_f3_p.png" alt="Prédiction GP f3" class="plot-img">
            <p class="caption"><strong>Figure 11 :</strong> Reconstruction GP isolant le pic central.</p>
        </div>
        <div class="fig-box">
            <img src="img/nn_2d_f3_p.png" alt="Prédiction CNN f3" class="plot-img">
            <p class="caption"><strong>Figure 12 :</strong> Approximation CNN 2D de la fonction hybride.</p>
        </div>
    </div>

    <p>Cette fonction mélangeant oscillations et singularité centrale met en évidence la supériorité du GP (Figure 11),
        qui parvient à isoler le pic exponentiel tout en stabilisant le relief périphérique. La <strong>Figure
            12</strong> montre que le CNN échoue presque totalement à stabiliser la surface : le pic central est déformé
        et le relief environnant est noyé dans un bruit visuel important. Le réseau semble "perdu" entre la gestion du
        pic de forte amplitude et les micro-oscillations, produisant un rendu chaotique et inutilisable pour une
        cartographie de précision.</p>

    <h4 id="synthese-2d">2.3.5 Synthèse comparative des performances en 2D</h4>

    <p>Le tableau ci-dessous regroupe les métriques de performance obtenues pour les deux modèles sur les trois surfaces
        de test. Cette comparaison quantitative permet de valider les observations visuelles faites précédemment sur la
        fidélité de la reconstruction.</p>

    <table class="small-table">
        <thead>
            <tr>
                <th>Fonction</th>
                <th>Modèle</th>
                <th>MSE (Erreur)</th>
                <th>Score $R^2$</th>
                <th>Incertitude moyenne (GP)</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td rowspan="2">$f_1$ : cosinus simple</td>
                <td><strong>Processus Gaussien</strong></td>
                <td>0.0002</td>
                <td>0.9996</td>
                <td>0.0090</td>
            </tr>
            <tr>
                <td>Réseau CNN</td>
                <td>0.0507</td>
                <td>0.8986</td>
                <td>/</td>
            </tr>
            <tr>
                <td rowspan="2">$f_2$ : boîte à œufs</td>
                <td><strong>Processus Gaussien</strong></td>
                <td>0.0001</td>
                <td>0.9996</td>
                <td>0.0056</td>
            </tr>
            <tr>
                <td>Réseau CNN</td>
                <td>0.0941</td>
                <td>0.6233</td>
                <td>/</td>
            </tr>
            <tr>
                <td rowspan="2">$f_3$ : pic + oscillations</td>
                <td><strong>Processus Gaussien</strong></td>
                <td>0.0037</td>
                <td>0.9874</td>
                <td>0.0191</td>
            </tr>
            <tr>
                <td>Réseau CNN</td>
                <td>0.1462</td>
                <td>0.4978</td>
                <td>/</td>
            </tr>
        </tbody>
    </table>

    <p>L'analyse de ces résultats confirme que le <strong>Processus Gaussien (GP)</strong> reste le modèle le plus
        performant pour la modélisation en dimension 2. Sur les fonctions $f_1$ et $f_2$, le GP atteint une précision
        quasi parfaite avec des scores $R^2$ de 0,9996, tandis que le réseau CNN montre des signes de faiblesse,
        notamment sur la fonction $f_2$ où son score chute à 0,6233.</p>

    <p>Le cas de la fonction complexe $f_3$ est le plus révélateur : alors que le Processus Gaussien maintient une
        excellente capacité d'approximation ($R^2 = 0,9874$), les performances du réseau CNN s'effondrent avec un score
        de 0,4978 et une erreur (MSE) nettement plus élevée (0,1462 contre 0,0037 pour le GP). Cette différence
        s'explique par la capacité du GP à gérer nativement les corrélations spatiales et les changements d'échelle via
        son noyau, là où le CNN peine à généraliser à partir de coordonnées brutes sur des signaux complexes.</p>

    <p>En conclusion de cette étude bidimensionnelle, le Processus Gaussien s'impose à nouveau comme l'outil de
        référence. Sa capacité à fournir une estimation de l'incertitude (très faible ici, entre 0,005 et 0,019) offre
        une garantie supplémentaire sur la fiabilité de la cartographie forestière générée, ce que ne permet pas
        l'architecture CNN utilisée.</p>

    <div class="transition-box">
        <p><strong>Transition vers l'application forestière :</strong> Cette étude comparative sur fonctions simulées a
            démontré la robustesse des Processus Gaussiens (GP) pour modéliser des signaux complexes avec une grande
            fluidité et une gestion fiable de l'incertitude. Forts de ces résultats théoriques, nous allons désormais
            appliquer ces modèles à des données forestières réelles. L'enjeu sera d'évaluer si la supériorité du GP se
            confirme face au bruit et à la variance de séries temporelles pluriannuelles (température et humidité), afin
            de fournir des prédictions robustes pour la préservation de ces écosystèmes fragiles.</p>
    </div>



    <footer>
        <div class="footer-content">
            <p>Polytech Clermont-Ferrand — Département Ingénierie Mathématique et Data Science (IMDS)</p>
            <p>Projet 5A — Année Universitaire 2025 / 2026</p>
        </div>
    </footer>

</body>

</html>