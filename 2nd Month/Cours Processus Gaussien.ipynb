{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d700a486",
   "metadata": {},
   "source": [
    "# üß† Cours complet : Les Processus Gaussiens (GP)\n",
    "\n",
    "---\n",
    "\n",
    "## üìò 1. Introduction\n",
    "\n",
    "Un **processus gaussien (GP)** est une g√©n√©ralisation du concept de **loi normale** aux **fonctions continues**.\n",
    "\n",
    "Alors qu‚Äôune loi normale d√©crit la distribution d‚Äôun **scalaire al√©atoire** ou d‚Äôun **vecteur al√©atoire** fini,  \n",
    "un processus gaussien d√©crit une **distribution sur des fonctions**.\n",
    "\n",
    "Autrement dit :\n",
    "\n",
    "> Un processus gaussien est une distribution de probabilit√© sur l‚Äôensemble des fonctions continues.  \n",
    "> Il permet de dire ¬´ quelle forme une fonction peut prendre ¬ª, tout en mesurant l‚Äôincertitude associ√©e.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Les processus gaussiens (GP) sont une m√©thode d'apprentissage supervis√© non param√©trique utilis√©e pour r√©soudre les probl√®mes de r√©gression et de classification probabiliste.\n",
    "\n",
    "Les avantages des processus gaussiens sont :\n",
    "\n",
    "- La pr√©diction interpole les observations (au moins pour les noyaux r√©guliers).\n",
    "- La pr√©diction est probabiliste (gaussienne) de sorte que l'on peut calculer des intervalles de confiance empiriques et d√©cider en fonction de ceux-ci si l'on doit r√©ajuster (ajustement en ligne, ajustement adaptatif) la pr√©diction dans une r√©gion d'int√©r√™t.\n",
    "- Polyvalent : diff√©rents kernels peuvent √™tre sp√©cifi√©s. Des noyaux courants sont fournis, mais il est √©galement possible de sp√©cifier des noyaux personnalis√©s.\n",
    "\n",
    "Les inconv√©nients des processus gaussiens incluent :\n",
    "\n",
    "- Notre impl√©mentation n'est pas √©parse, c'est-√†-dire qu'elle utilise l'ensemble des informations sur les √©chantillons/caract√©ristiques pour effectuer la pr√©diction.\n",
    "- Ils perdent en efficacit√© dans les espaces de grande dimension, notamment lorsque le nombre de caract√©ristiques d√©passe quelques dizaines.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© 2. D√©finition formelle\n",
    "\n",
    "On note un processus gaussien comme :\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))\n",
    "$$\n",
    "\n",
    "o√π :\n",
    "\n",
    "- $m(x) = \\mathbb{E}[f(x)]$ est la **fonction moyenne**\n",
    "- $k(x, x') = \\mathrm{Cov}(f(x), f(x'))$ est la **fonction de covariance** (ou **kernel**)\n",
    "\n",
    "Pour tout ensemble fini de points $X = [x_1, \\dots, x_n]$,  \n",
    "le vecteur al√©atoire $f = [f(x_1), \\dots, f(x_n)]^T$ suit une **loi normale multivari√©e** :\n",
    "\n",
    "$$\n",
    "f \\sim \\mathcal{N}(m(X), K(X, X))\n",
    "$$\n",
    "\n",
    "avec :\n",
    "\n",
    "$$\n",
    "m(X) = [m(x_1), \\dots, m(x_n)]^T, \\quad K(X, X)_{ij} = k(x_i, x_j)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ 3. Cas unidimensionnel (1D)\n",
    "\n",
    "Dans le cas 1D, on suppose que la fonction $f : \\mathbb{R} \\to \\mathbb{R}$ est un processus gaussien.\n",
    "\n",
    "Chaque paire de points $(x_i, x_j)$ est corr√©l√©e selon la covariance $k(x_i, x_j)$.\n",
    "\n",
    "La distribution jointe sur $n$ points est alors :\n",
    "\n",
    "$$\n",
    "p(f(X)) = \\frac{1}{(2\\pi)^{n/2} |K|^{1/2}}\n",
    "\\exp\\left(\n",
    "-\\frac{1}{2}(f(X) - m(X))^T K^{-1} (f(X) - m(X))\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "o√π $K = K(X, X)$.\n",
    "\n",
    "---\n",
    "\n",
    "## üîä 4. Cas avec bruit d‚Äôobservation\n",
    "\n",
    "Dans un probl√®me r√©el, les observations $y_i$ sont bruit√©es :\n",
    "\n",
    "$$\n",
    "y_i = f(x_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\n",
    "$$\n",
    "\n",
    "Cela signifie que la variance observ√©e contient le **bruit de mesure** :\n",
    "\n",
    "$$\n",
    "y \\sim \\mathcal{N}\\big(m(X), K(X, X) + \\sigma_n^2 I\\big)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üìà 5. Pr√©diction d‚Äôun nouveau point $x_*$\n",
    "\n",
    "On veut conna√Ætre la distribution de $f_* = f(x_*)$ conditionnellement aux observations $(X, y)$.\n",
    "\n",
    "On d√©finit :\n",
    "\n",
    "$$\n",
    "k_* = k(X, x_*) \\in \\mathbb{R}^n, \\quad\n",
    "k_{**} = k(x_*, x_*), \\quad\n",
    "K_y = K(X, X) + \\sigma_n^2 I\n",
    "$$\n",
    "\n",
    "La loi jointe entre $y$ et $f_*$ est :\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y \\\\\n",
    "f_*\n",
    "\\end{bmatrix}\n",
    "\\sim\n",
    "\\mathcal{N}\\left(\n",
    "\\begin{bmatrix}\n",
    "m(X) \\\\\n",
    "m(x_*)\n",
    "\\end{bmatrix},\n",
    "\\begin{bmatrix}\n",
    "K_y & k_* \\\\\n",
    "k_*^T & k_{**}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "En conditionnant sur $y$, on obtient la loi a posteriori :\n",
    "\n",
    "$$\n",
    "f_* \\,|\\, X, y, x_* \\sim \\mathcal{N}(\\mu_*, \\sigma_*^2)\n",
    "$$\n",
    "\n",
    "avec :\n",
    "\n",
    "$$\n",
    "\\mu_* = m(x_*) + k_*^T K_y^{-1} (y - m(X))\n",
    "$$\n",
    "\n",
    "et\n",
    "\n",
    "$$\n",
    "\\sigma_*^2 = k_{**} - k_*^T K_y^{-1} k_*\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ 6. Fonction de vraisemblance marginale\n",
    "\n",
    "Le mod√®le complet induit une **distribution sur les observations** :\n",
    "\n",
    "$$\n",
    "p(y | X, \\theta) = \\mathcal{N}\\big(0, K_\\theta + \\sigma_n^2 I\\big)\n",
    "$$\n",
    "\n",
    "La **log-vraisemblance marginale** s‚Äô√©crit :\n",
    "\n",
    "$$\n",
    "\\log p(y|X,\\theta) =\n",
    "-\\frac{1}{2} y^T (K_\\theta + \\sigma_n^2 I)^{-1} y\n",
    "-\\frac{1}{2} \\log |K_\\theta + \\sigma_n^2 I|\n",
    "-\\frac{n}{2} \\log(2\\pi)\n",
    "$$\n",
    "\n",
    "En pratique, on maximise cette fonction pour trouver les **hyperparam√®tres optimaux** :\n",
    "\n",
    "$$\n",
    "\\theta = \\{ \\ell, \\sigma_f, \\sigma_n \\}\n",
    "$$\n",
    "\n",
    "o√π :\n",
    "\n",
    "- $\\ell$ = longueur caract√©ristique (lisse ou non)\n",
    "- $\\sigma_f$ = amplitude du signal\n",
    "- $\\sigma_n$ = niveau de bruit\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851b3435",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "455f043c",
   "metadata": {},
   "source": [
    "## Fonctions de covariance (Kernels) dans les Processus Gaussiens \n",
    "\n",
    "Les **noyaux (ou fonctions de covariance)** d√©finissent la structure de d√©pendance et les propri√©t√©s de lissage d‚Äôun **processus gaussien (GP)**.  \n",
    "Le choix du noyau est **crucial** : il d√©termine la mani√®re dont le mod√®le g√©n√©ralise et la forme des fonctions qu‚Äôil peut apprendre.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è R√¥le du noyau\n",
    "Un **noyau** $k(x, x')$ mesure la **similarit√©** entre deux points d‚Äôentr√©e $x$ et $x'$.  \n",
    "C‚Äôest lui qui encode nos **hypoth√®ses a priori** sur la fonction $f(x)$ :\n",
    "- sa **lissit√©**,\n",
    "- sa **p√©riodicit√©**,\n",
    "- sa **lin√©arit√©**,\n",
    "- ou encore la mani√®re dont elle **varie localement**.\n",
    "\n",
    "Un mauvais choix de noyau conduit souvent √† un mod√®le mal calibr√©, qui surapprend ou sous-apprend.\n",
    "\n",
    "---\n",
    "\n",
    "### üß© Noyaux standards\n",
    "\n",
    "#### 1. Noyau exponentiel quadratique (Squared Exponential ou RBF)\n",
    "$$\n",
    "k_{\\text{SE}}(x, x') = \\sigma^2 \\exp\\!\\left( -\\frac{(x - x')^2}{2\\ell^2} \\right)\n",
    "$$\n",
    "- Tr√®s utilis√© car **universel** et **infinitement d√©rivable**.  \n",
    "- Param√®tres :\n",
    "  - $\\ell$ : *longueur de corr√©lation*, contr√¥le la largeur des \"oscillations\" (plus $\\ell$ est petit, plus la fonction varie vite)  \n",
    "  - $\\sigma^2$ : *variance de sortie*, amplitude moyenne des fluctuations.  \n",
    "- Id√©al pour des fonctions **douces et continues**.  \n",
    "- ‚ö†Ô∏è Moins adapt√© pour des fonctions avec discontinuit√©s ou changements brusques.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Noyau quadratique rationnel (Rational Quadratic)\n",
    "$$\n",
    "k_{\\text{RQ}}(x, x') = \\sigma^2 \\left( 1 + \\frac{(x - x')^2}{2 \\alpha \\ell^2} \\right)^{-\\alpha}\n",
    "$$\n",
    "- Mod√©lise une **superposition** de plusieurs RBF √† diff√©rentes √©chelles.  \n",
    "- $\\alpha$ contr√¥le le poids relatif entre les grandes et petites √©chelles :\n",
    "  - $\\alpha \\to \\infty$ : √©quivalent au SE kernel.  \n",
    "- Bon pour des ph√©nom√®nes √† **plusieurs niveaux de variabilit√©**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. Noyau p√©riodique\n",
    "$$\n",
    "k_{\\text{Per}}(x, x') = \\sigma^2 \\exp\\!\\left( -\\frac{2\\sin^2(\\pi |x - x'| / p)}{\\ell^2} \\right)\n",
    "$$\n",
    "- Repr√©sente des fonctions **strictement p√©riodiques**.  \n",
    "- Param√®tres :\n",
    "  - $p$ : p√©riode du motif r√©p√©t√©,  \n",
    "  - $\\ell$ : lissage local.  \n",
    "- Exemple : variation saisonni√®re, signaux cycliques, etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. Noyau p√©riodique local (Local Periodic)\n",
    "$$\n",
    "k_{\\text{LocalPer}}(x, x') = k_{\\text{Per}}(x, x') \\cdot k_{\\text{SE}}(x, x')\n",
    "$$\n",
    "- Combine p√©riodicit√© + √©volution lente du motif.  \n",
    "- Repr√©sente des fonctions **p√©riodiques mais √©volutives** (ex. climat changeant d‚Äôune ann√©e √† l‚Äôautre).\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. Noyau lin√©aire\n",
    "$$\n",
    "k_{\\text{Lin}}(x, x') = \\sigma_b^2 + \\sigma_v^2 (x - c)(x' - c)\n",
    "$$\n",
    "- Correspond √† une **r√©gression lin√©aire bay√©sienne**.  \n",
    "- Non stationnaire (d√©pend de la position absolue).  \n",
    "- Param√®tres :\n",
    "  - $c$ : centre de la r√©gression (origine),\n",
    "  - $\\sigma_b^2$, $\\sigma_v^2$ : contr√¥lent le biais et la variance des pentes.  \n",
    "- Id√©al pour des **tendances globales**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Combinaison de noyaux\n",
    "\n",
    "On peut **composer** des noyaux pour capturer plusieurs comportements √† la fois.\n",
    "\n",
    "#### ‚ûï Somme de noyaux\n",
    "$$\n",
    "k(x,x') = k_1(x,x') + k_2(x,x')\n",
    "$$\n",
    "- Interpr√©tation : **OR logique** ‚Üí le GP combine des comportements additifs.  \n",
    "- Exemple : $k_{\\text{SE}} + k_{\\text{Per}}$ ‚Üí fonction lisse + composante p√©riodique.\n",
    "\n",
    "#### ‚úñÔ∏è Produit de noyaux\n",
    "$$\n",
    "k(x,x') = k_1(x,x') \\times k_2(x,x')\n",
    "$$\n",
    "- Interpr√©tation : **AND logique** ‚Üí les similarit√©s doivent exister dans les deux composantes.  \n",
    "- Exemple : $k_{\\text{Per}} \\times k_{\\text{SE}}$ ‚Üí p√©riodique mais avec motifs qui √©voluent lentement.\n",
    "\n",
    "---\n",
    "\n",
    "### üßÆ Propri√©t√©s utiles\n",
    "- **Stationnarit√©** : $k(x, x') = k(x - x')$ d√©pend seulement de la distance ‚Üí invariant par translation.  \n",
    "- **Isotropie** : d√©pend uniquement de $||x - x'||$.  \n",
    "- **Non stationnaire** : d√©pend de $x$ et $x'$ s√©par√©ment (ex : lin√©aire).\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Interpr√©tation pratique\n",
    "\n",
    "| Type de ph√©nom√®ne | Noyau recommand√© | Exemple concret |\n",
    "|--------------------|------------------|-----------------|\n",
    "| Lisse et continu | SE | Temp√©rature quotidienne |\n",
    "| Multi-√©chelle | RQ | March√©s financiers |\n",
    "| P√©riodique | Periodic | Donn√©es saisonni√®res |\n",
    "| P√©riodique + d√©rive lente | Local Periodic | Cycle climatique changeant |\n",
    "| Lin√©aire | Linear | Croissance d‚Äôune plante |\n",
    "| Mixte | Somme/Produit | Signal compos√© (tendance + bruit + saisonnalit√©) |\n",
    "\n",
    "---\n",
    "\n",
    "### üåç Autres extensions\n",
    "\n",
    "- **Variables cat√©gorielles** : utiliser un *one-hot encoding* et un RBF sur chaque dimension.  \n",
    "- **Sym√©trie connue** : on peut forcer $f(x) = f(-x)$ en ajoutant un noyau sym√©tris√© :  \n",
    "  $k_{\\text{sym}}(x,x') = k(x,x') + k(-x,x')$  \n",
    "- **Structure faible dimensionnelle** :  \n",
    "  $k(x,x') = k(Ax, A x')$, avec $A$ une matrice de projection bas-dimensionnelle.\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ En r√©sum√©\n",
    "- Le **noyau est le c≈ìur du GP** : il d√©finit le comportement de la fonction a priori.  \n",
    "- Il faut **choisir ou construire** un noyau selon la **structure** du ph√©nom√®ne √©tudi√©.  \n",
    "- Les combinaisons de noyaux permettent de mod√©liser des fonctions **complexes, mixtes, ou hi√©rarchiques**.  \n",
    "- L‚Äôoptimisation des **hyperparam√®tres** du noyau (via la log-vraisemblance marginale) ajuste automatiquement la flexibilit√© du mod√®le.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d63f4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "300e2e7b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## üåç 8. Extension en dimension n\n",
    "\n",
    "Pour un espace d‚Äôentr√©e $x \\in \\mathbb{R}^d$ :\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))\n",
    "$$\n",
    "\n",
    "avec par exemple le **RBF multidimensionnel** :\n",
    "\n",
    "$$\n",
    "k(x, x') = \\sigma_f^2 \\exp\\left(-\\frac{1}{2}(x - x')^T \\Lambda^{-1} (x - x')\\right)\n",
    "$$\n",
    "\n",
    "o√π :\n",
    "\n",
    "$$\n",
    "\\Lambda = \\text{diag}(\\ell_1^2, \\dots, \\ell_d^2)\n",
    "$$\n",
    "\n",
    "Chaque dimension a sa propre **longueur caract√©ristique**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç 9. Interpr√©tation g√©om√©trique\n",
    "\n",
    "Un processus gaussien peut √™tre vu comme un **ensemble infini de fonctions** tir√©es d‚Äôune distribution conjointe.\n",
    "\n",
    "Chaque fonction $f$ tir√©e d‚Äôun GP :\n",
    "\n",
    "- respecte la moyenne $m(x)$,\n",
    "- pr√©sente une ¬´ forme ¬ª compatible avec le kernel $k$.\n",
    "\n",
    "Les zones avec peu de donn√©es ont une **incertitude √©lev√©e**,  \n",
    "et celles avec beaucoup de points proches ont une **incertitude faible**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìä 10. Applications concr√®tes\n",
    "\n",
    "Les processus gaussiens sont utilis√©s dans :\n",
    "\n",
    "| Domaine | Utilisation typique |\n",
    "|----------|--------------------|\n",
    "| **R√©gression** | Mod√©liser des relations non lin√©aires avec incertitude |\n",
    "| **Optimisation bay√©sienne** | Chercher un optimum co√ªteux √† √©valuer |\n",
    "| **G√©ostatistique (krigeage)** | Interpolation spatiale en g√©osciences |\n",
    "| **Surrogate modeling** | Approximation de mod√®les co√ªteux (physique, CFD, etc.) |\n",
    "| **Climatologie / √©cologie** | Mod√©liser des ph√©nom√®nes spatiaux comme la croissance d‚Äôesp√®ces |\n",
    "| **Machine Learning bay√©sien** | Apprentissage probabiliste interpr√©table |\n",
    "\n",
    "---\n",
    "\n",
    "## üå≥ 11. Exemple conceptuel : croissance d‚Äôesp√®ces d‚Äôarbres\n",
    "\n",
    "On suppose que la **hauteur moyenne d‚Äôun arbre** d√©pend :\n",
    "\n",
    "- de son **√¢ge** $x_1$,\n",
    "- de la **temp√©rature moyenne annuelle** $x_2$.\n",
    "\n",
    "Le GP permet de mod√©liser :\n",
    "\n",
    "$$\n",
    "h(x_1, x_2) \\sim \\mathcal{GP}(m(x_1, x_2), k((x_1, x_2), (x_1', x_2')))\n",
    "$$\n",
    "\n",
    "o√π :\n",
    "\n",
    "- $m(x_1, x_2)$ peut √™tre pris nul ou lin√©aire  \n",
    "- $k$ d√©crit la similarit√© √©cologique entre deux environnements\n",
    "\n",
    "Cela permet :\n",
    "\n",
    "- d‚Äôestimer la croissance attendue sous diff√©rents climats,\n",
    "- de quantifier l‚Äôincertitude dans les zones peu observ√©es,\n",
    "- d‚Äôinterpoler entre r√©gions (krigeage √©cologique).\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 12. R√©sum√© final\n",
    "\n",
    "| √âl√©ment | R√¥le |\n",
    "|----------|------|\n",
    "| $m(x)$ | Moyenne du processus |\n",
    "| $k(x, x')$ | Covariance (similarit√©) |\n",
    "| $\\sigma_n^2$ | Variance du bruit |\n",
    "| $\\mu_*(x_*)$ | Moyenne pr√©dictive |\n",
    "| $\\sigma_*^2(x_*)$ | Incertitude pr√©dictive |\n",
    "| Log-vraisemblance | Ajustement des hyperparam√®tres |\n",
    "| Kernel | Encode la structure spatiale ou temporelle des donn√©es |\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ 13. √Ä retenir\n",
    "\n",
    "> üîπ Un **processus gaussien** est une distribution sur les fonctions.  \n",
    "> üîπ Il permet de **mod√©liser des ph√©nom√®nes continus incertains** de mani√®re non param√©trique.  \n",
    "> üîπ Il combine **interpr√©tabilit√©**, **rigueur bay√©sienne** et **incertitude explicite**.  \n",
    "> üîπ Son c≈ìur est le **kernel**, qui traduit nos hypoth√®ses sur la r√©gularit√© du monde r√©el.  \n",
    "> üîπ En pratique, il est fondamental pour la **r√©gression probabiliste**, l‚Äô**optimisation bay√©sienne** et la **mod√©lisation spatiale**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a855f01c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9a51c7d",
   "metadata": {},
   "source": [
    "# Python\n",
    "\n",
    "## R√©gression par processus gaussien (RPG)\n",
    "\n",
    "La **r√©gression par processus gaussien (Gaussian Process Regression, GPR)** est impl√©ment√©e en Python via `GaussianProcessRegressor` de `scikit-learn`. Elle permet de r√©aliser des pr√©dictions probabilistes en fournissant √† la fois la **moyenne** et l'**√©cart-type** de la pr√©diction.\n",
    "\n",
    "---\n",
    "\n",
    "## Principe g√©n√©ral\n",
    "\n",
    "- Un **processus gaussien (PG)** est d√©fini par :\n",
    "  - une **fonction a priori** (mean function) : constante et nulle (`normalize_y=False`) ou √©gale √† la moyenne des donn√©es (`normalize_y=True`),\n",
    "  - une **fonction de covariance** sp√©cifi√©e par un **noyau (`kernel`)**.\n",
    "\n",
    "- Le mod√®le combine cette fonction a priori avec la **vraisemblance des donn√©es d'apprentissage**.\n",
    "\n",
    "---\n",
    "\n",
    "## Optimisation des hyperparam√®tres\n",
    "\n",
    "- Les hyperparam√®tres du noyau sont ajust√©s en maximisant la **vraisemblance logarithmique marginale (LML)** via l'**optimiseur** transmis √† `GaussianProcessRegressor`.\n",
    "- Pour √©viter les **optima locaux**, l'optimisation peut √™tre relanc√©e plusieurs fois (`n_restarts_optimizer`) √† partir de valeurs al√©atoires dans les bornes autoris√©es.\n",
    "- Si on ne souhaite **pas optimiser** les hyperparam√®tres initiaux, on peut passer `optimizer=None`.\n",
    "\n",
    "---\n",
    "\n",
    "## Gestion du bruit\n",
    "\n",
    "- Le niveau de bruit des cibles peut √™tre :\n",
    "  - un **scalaire global**,\n",
    "  - un **vecteur sp√©cifique √† chaque point** via `alpha`.\n",
    "- Le bruit agit comme une **r√©gularisation de Tikhonov**, ajout√© √† la diagonale de la matrice du noyau pour stabiliser l'ajustement.\n",
    "- Alternative : inclure un **WhiteKernel** dans le noyau pour estimer le bruit global automatiquement.\n",
    "\n",
    "---\n",
    "\n",
    "## Fonctionnalit√©s suppl√©mentaires\n",
    "\n",
    "- Pr√©diction **sans ajustement pr√©alable** (bas√©e uniquement sur le GP a priori).\n",
    "- M√©thode `sample_y(X)` : √©chantillonne les valeurs du GP (a priori ou a posteriori).\n",
    "- M√©thode `log_marginal_likelihood(theta)` : permet de tester d'autres m√©thodes d'optimisation des hyperparam√®tres (ex. MCMC).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0109505c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb07a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df8b731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77f9443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f87e68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f48b43d",
   "metadata": {},
   "source": [
    "## üåê Processus Gaussien en 2D (et en dimensions sup√©rieures)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Intuition g√©n√©rale\n",
    "\n",
    "Un **Processus Gaussien (GP)** d√©finit une **distribution sur les fonctions**.  \n",
    "On √©crit :\n",
    "$$\n",
    "f(\\mathbf{x}) \\sim \\mathcal{GP}(m(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}'))\n",
    "$$\n",
    "\n",
    "o√π :\n",
    "- $m(\\mathbf{x}) = \\mathbb{E}[f(\\mathbf{x})]$ est la **fonction moyenne**,  \n",
    "- $k(\\mathbf{x}, \\mathbf{x}') = \\text{Cov}(f(\\mathbf{x}), f(\\mathbf{x}'))$ est la **fonction de covariance (kernel)**.\n",
    "\n",
    "En dimension 1, $\\mathbf{x}$ est un scalaire.  \n",
    "En **dimension 2**, $\\mathbf{x} = (x_1, x_2)$ repr√©sente un **point dans le plan**,  \n",
    "et $f(x_1, x_2)$ repr√©sente une **surface al√©atoire**.\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ Processus Gaussien en 2D : la vision g√©om√©trique\n",
    "\n",
    "Un GP 2D suppose que pour **tout ensemble fini de points d‚Äôentr√©e**  \n",
    "$\\{\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_n\\}$,  \n",
    "le vecteur al√©atoire des valeurs :\n",
    "$$\n",
    "\\mathbf{f} = [f(\\mathbf{x}_1), f(\\mathbf{x}_2), \\dots, f(\\mathbf{x}_n)]^\\top\n",
    "$$\n",
    "suit une **distribution normale multivari√©e** :\n",
    "$$\n",
    "\\mathbf{f} \\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{K})\n",
    "$$\n",
    "\n",
    "avec :\n",
    "- $\\mathbf{m} = [m(\\mathbf{x}_1), \\dots, m(\\mathbf{x}_n)]^\\top$  \n",
    "- $\\mathbf{K}_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$  \n",
    "\n",
    "Ainsi, dans le cas 2D :\n",
    "- Chaque point $(x_1, x_2)$ a une **valeur al√©atoire corr√©l√©e** √† ses voisins,\n",
    "- Le **kernel** contr√¥le la *forme*, *la r√©gularit√©*, et *l‚Äô√©tendue spatiale* de cette corr√©lation.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Exemple d‚Äôinterpr√©tation visuelle\n",
    "\n",
    "- Si $k$ est un **RBF kernel**, les points proches dans le plan $(x_1, x_2)$ auront des valeurs $f(x_1, x_2)$ similaires ‚Üí surface lisse.  \n",
    "- Si $k$ est un **p√©riodique**, on obtient une surface avec des motifs r√©p√©titifs.  \n",
    "- Si $k$ est **lin√©aire**, la surface sera un plan (ou l√©g√®rement inclin√©e selon les param√®tres).\n",
    "\n",
    "En Python, on peut visualiser un GP 2D comme une **carte de chaleur** ou une **surface 3D** :\n",
    "- l‚Äôaxe horizontal ‚Üí $x_1$  \n",
    "- l‚Äôaxe vertical ‚Üí $x_2$  \n",
    "- la hauteur (ou couleur) ‚Üí $f(x_1, x_2)$\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Ce qu‚Äôil se passe en pratique (dans Python)\n",
    "\n",
    "#### üßÆ √âtapes internes (ex : `sklearn.gaussian_process` ou `GPy`)\n",
    "\n",
    "1. **Cr√©ation du kernel**  \n",
    "   Exemple : `kernel = RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1)`\n",
    "\n",
    "2. **Construction de la matrice de covariance**\n",
    "   $$\n",
    "   K(X, X) = \\begin{bmatrix}\n",
    "   k(x_1, x_1) & \\cdots & k(x_1, x_n) \\\\\n",
    "   \\vdots & \\ddots & \\vdots \\\\\n",
    "   k(x_n, x_1) & \\cdots & k(x_n, x_n)\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "3. **Calcul du mod√®le a posteriori**  \n",
    "   Pour une observation bruit√©e :\n",
    "   $$\n",
    "   y_i = f(\\mathbf{x}_i) + \\varepsilon_i, \\quad \\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)\n",
    "   $$\n",
    "   on obtient :\n",
    "   $$\n",
    "   p(f_* | X, y, X_*) = \\mathcal{N}(\\mu_*, \\Sigma_*)\n",
    "   $$\n",
    "   avec :\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\mu_* &= K(X_*, X)[K(X, X) + \\sigma_n^2 I]^{-1} y \\\\\n",
    "   \\Sigma_* &= K(X_*, X_*) - K(X_*, X)[K(X, X) + \\sigma_n^2 I]^{-1} K(X, X_*)\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "4. **Optimisation automatique des hyperparam√®tres**  \n",
    "   Les param√®tres du kernel ($\\ell, \\sigma^2, \\sigma_n^2$, etc.) sont ajust√©s en **maximisant la log-vraisemblance marginale** :\n",
    "   $$\n",
    "   \\log p(y | X) = -\\frac{1}{2} y^\\top [K(X,X) + \\sigma_n^2 I]^{-1} y - \\frac{1}{2}\\log|K(X,X) + \\sigma_n^2 I| - \\frac{n}{2}\\log(2\\pi)\n",
    "   $$\n",
    "\n",
    "   üëâ Cela permet d‚Äôadapter la **complexit√© du mod√®le** √† la structure r√©elle des donn√©es.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä En 2D et au-del√† (n dimensions)\n",
    "\n",
    "Le principe reste le m√™me :  \n",
    "$\\mathbf{x} \\in \\mathbb{R}^d$, le noyau $k(\\mathbf{x}, \\mathbf{x}')$ d√©finit la corr√©lation entre les points dans cet espace $d$-dimensionnel.\n",
    "\n",
    "- En 3D ‚Üí on apprend un **volume de corr√©lations**  \n",
    "- En nD ‚Üí on apprend une **surface implicite** dans un espace de grande dimension.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Interpr√©tation pratique en 2D\n",
    "\n",
    "| Type de surface observ√©e | Noyau typique | Interpr√©tation |\n",
    "|----------------------------|----------------|----------------|\n",
    "| Surface lisse | RBF | terrain, temp√©rature, pression atmosph√©rique |\n",
    "| Motif p√©riodique | Periodic | vagues, climat, cycles biologiques |\n",
    "| Gradient global | Linear | plan inclin√©, tendance spatiale |\n",
    "| Surface √©voluant localement | Local Periodic ou Sum | paysage avec motifs + bruit local |\n",
    "\n",
    "---\n",
    "\n",
    "### üí° En r√©sum√©\n",
    "\n",
    "- Le GP 2D mod√©lise une **surface al√©atoire corr√©l√©e** dans le plan.  \n",
    "- Le **kernel** d√©finit la *texture* et la *r√©gularit√©* de cette surface.  \n",
    "- En Python :\n",
    "  - On construit un kernel adapt√© (RBF, Matern, etc.),\n",
    "  - On entra√Æne le mod√®le (fit),\n",
    "  - On pr√©dit sur une grille 2D (predict),\n",
    "  - Et on visualise la **moyenne pr√©dite** et **l‚Äôincertitude** sous forme de carte ou de surface.\n",
    "\n",
    "---\n",
    "\n",
    "üìò **√Ä retenir :**\n",
    "> Un GP ne pr√©dit pas une valeur, il pr√©dit **une distribution** sur les fonctions possibles.  \n",
    "> En 2D ou nD, cela revient √† pr√©dire une **surface probabiliste**, o√π chaque point a une moyenne et une variance qui refl√®te notre **confiance** dans la pr√©diction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4029480d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
